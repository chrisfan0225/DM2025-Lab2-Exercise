{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: 范辰于\n",
    "\n",
    "Student ID: 112070034\n",
    "\n",
    "GitHub ID: chrisfan0225\n",
    "\n",
    "Kaggle name: Chris Fan\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![private_ranking.png](./pics/private_ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "For this competition, I established a consistent and reproducible preprocessing pipeline to transform raw tweets into usable features for both classical machine learning models and modern Transformer-based architectures. I began by applying several normalization steps to reduce noise and standardize the data. All text was **converted to lowercase** to avoid vocabulary fragmentation, and unnecessary whitespace was removed. Depending on the downstream model, I selectively preserved or removed special tokens: for traditional models such as TF-IDF, Word2Vec, and PAMI, I used a cleaner version of the text, while for Transformer models (BERT, RoBERTa, DeBERTa) I **kept URLs, emojis, and punctuation** because these models are trained to handle such symbols through their subword tokenizers.\n",
    "\n",
    "Tokenization followed a dual strategy. For TF-IDF, Word2Vec, and PAMI, I adopted simple **whitespace tokenization** to ensure transparency and speed. Also, I tried to remove frequency top 4% and bottom 1% words just like what we did in Lab 1, in order to reduce noises or stop words. For PAMI specifically, duplicated tokens were removed when constructing transactions for frequent pattern mining. In contrast, the Transformer family relied on **HuggingFace’s subword tokenizers**, which allowed emotional nuances and short informal expressions to be captured more accurately. This separation ensured that each model received the most appropriate representation of the text.\n",
    "\n",
    "Because the dataset exhibited **clear class imbalance**: particularly with **joy** appearing notably more often than the other classes. I used **stratified sampling or class_weight** in every train–validation split to preserve label distribution. I evaluated all models using **macro-F1** to avoid majority-class bias, and I regularly inspected per-class predictions to ensure no model collapsed toward predicting only the majority emotion. During experimentation, I also adopted a practical workflow: whenever I introduced a new model or a new type of feature, I first performed my own **80/20 train–test split** to get a quick, offline estimate of model behavior before spending submission attempts on Kaggle. This allowed me to rule out weak configurations early and iterate faster.\n",
    "\n",
    "For formal evaluation, I learned how to use **5-fold Stratified Cross-Validation**, which proved to be stable and predictive of actual leaderboard performance. For example, TF-IDF with LinearSVC achieved a CV score around 0.473, which aligned well with its public leaderboard score around 0.62, confirming that my validation procedure reflected true model generalization.\n",
    "\n",
    "Overall, my preprocessing strategy emphasized consistency, fairness, and reproducibility. Standardized normalization, model-appropriate tokenization, stratified splitting, CV validation, and a disciplined 80/20 offline testing workflow enabled me to compare models fairly and build stronger intuition for why certain features or architectures worked better than others during the development of this competition project.\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "To improve model performance, I gradually expanded my feature engineering pipeline from simple lexical features to richer semantic and structural representations. I began with **Bag-of-Words** and **TF-IDF**, which served as strong lexical baselines. TF-IDF with uni-grams and bi-grams captured emotionally salient keywords such as “hate”, “love”, or “so happy”, making it particularly effective for linear models. This feature alone produced highly competitive results, reaching 0.619 public leaderboard with LinearSVC, which confirmed that emotion classification in tweets is often keyword-driven.\n",
    "\n",
    "To incorporate semantic information beyond surface vocabulary, I experimented with Word2Vec embeddings. I evaluated both **average pooling** and **TF-IDF–weighted pooling**. Although these embeddings captured semantic similarity, they did not translate directly into emotional similarity: terms like “fire”, “burning”, and “lit” may be close in embedding space yet convey different emotional tones. As a result, pure Word2Vec models underperformed (around 0.56 LB), and even hybrid approaches such as Weighted W2V + LightGBM remained weaker than TF-IDF alone. This was really surprising to me.\n",
    "\n",
    "This gap can be explained by several factors. First, the commonly used average pooling strategy **compresses the entire sentence into a single dense vector**, which removes positional and intensity cues that are essential for emotion classification. Second, TF-IDF combined with linear models remains one of the strongest and most established approaches in text classification, and it captures high-impact emotional keywords far more effectively than dense embeddings. Third, Word2Vec relies on semantic similarity, but semantic closeness does not necessarily reflect emotional closeness—words like “fire”, “burning”, and “lit” may be near each other in embedding space yet express very different emotions depending on context. Finally, LinearSVC is not particularly well-suited for dense embeddings, which tend to produce highly entangled feature spaces where linear decision boundaries become harder to learn. These factors collectively explain why Word2Vec underperformed (≈0.56 LB) and why TF-IDF remained the dominant feature for classical machine learning models in this task.\n",
    "\n",
    "Building on these observations, I designed a feature concatenation scheme that combined multiple perspectives of the text. Specifically, I concatenated:\n",
    "\n",
    "(1) high-dimensional TF-IDF vectors (20k–40k dimensions),\n",
    "\n",
    "(2) weighted Word2Vec embeddings, and\n",
    "\n",
    "(3) a set of **hand-crafted statistical features** that I engineered manually.\n",
    "\n",
    "These hand-crafted features captured structural and stylistic cues often present in emotional tweets, including text length, word count, uppercase ratio, number of exclamation/question marks, elongated words (e.g.,“soooo”), repeated punctuation, ellipses, emoji frequency, presence of negations, use of first-/second-person pronouns, mentions (@), and hashtags (#). These features added a behavioral dimension to the representation, helping classical models detect emotional intensity, emphasis, or direct address. When concatenated with TF-IDF and Word2Vec, these hybrid features produced strong improvements, achieving 0.6269 LB with LinearSVC.\n",
    "\n",
    "To explore unsupervised structure in the data, I also extended Lab1’s approach by applying **PAMI through FP-Growth**. I constructed a transactions.txt file for the PAMI library, mined frequent word co-occurrence patterns (minimum support 3%), and analyzed common collocations such as (“not”, “happy”), (“feeling”, “down”), and (“so”, “excited”). Although these patterns were not directly used in the final models, they provided valuable insights into how emotions manifest through multi-word expressions rather than single tokens. These findings later guided my decision to prioritize models that capture contextual dependencies—namely, Transformer-based models.\n",
    "\n",
    "Overall, the feature engineering process evolved from simple lexical encoding to multi-level representations combining lexical, semantic, and structural components. Each stage contributed new insights, allowing me to understand which features strengthened which models and ultimately helping shape the design of my best-performing systems.\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "Throughout the project, I explored a wide range of models: from classical linear classifiers to deep neural networks and Transformer architectures, in order to understand how different modeling assumptions interact with the emotion classification task.\n",
    "\n",
    "I began with **traditional linear models**, including **Naive Bayes,Logistic Regression and LinearSVC**, trained on TF-IDF representations. These models were extremely stable and delivered strong performance, with **TF-IDF + LinearSVC** achieving 0.6234 public and 0.6198 private leaderboard scores, making it the **best performing non-neural approach**. The strength of linear models in this task comes from the fact that emotional polarity in short tweets is often determined by the presence of highly discriminative keywords, and TF-IDF effectively amplifies these signals. LinearSVC, in particular, benefited from **sparse high-dimensional representations** where linear decision boundaries work remarkably well.\n",
    "\n",
    "I then experimented with **tree-based and boosting models**, such as **XGBoost and LightGBM**. While I attempted training them on TF-IDF and dense Word2Vec embeddings, their performance plateaued around the 0.55–0.60 range. I expected these two models would outperform the linear ones, but the result showed that XGBoost struggled with the extremely high dimensionality and sparsity of TF-IDF vectors, leading to slower training and weaker generalization, while LightGBM worked slightly better on dense W2V embeddings but still could not match linear models. These results reinforced a known limitation: boosting methods are powerful for structured data but are generally less effective for sparse text vectors unless heavily engineered.\n",
    "\n",
    "To capture local phrase-level patterns, I implemented a **CNN-based text classifier** with an embedding layer followed by convolution + max pooling. The CNN performed noticeably better than Word2Vec-based classical models and effectively identified emotionally charged n-grams. However, its performance (public LB ≈ 0.5684) still lagged behind TF-IDF + SVM, I guess it's likely due to the limited dataset size and the difficulty of learning robust embeddings from scratch.\n",
    "\n",
    "The most substantial performance gains came from **Transformer-based models**, which can encode contextual, semantic, and emotional nuances at a much deeper level. I fine-tuned several architectures: including **BERT-base, RoBERTa-base and DeBERTa-v3 base**, and finally used partial freezing and voting (85–90%) to stabilize training and reduce overfitting. A key insight from experimentation was that replacing the Transformer’s classifier head with a LinearSVC trained on the extracted embeddings produced significantly more stable predictions than end-to-end fine-tuning. RoBERTa achieved around 0.69 on both public and private leaderboards, while **DeBERTa consistently outperformed all other models**, reaching 0.6997 public and 0.6922 private with an adjusted masking strategy that preserved more informative tokens during tokenization. This hybrid **DeBERTa + SVM** approach ultimately became my final submission, offering the best balance of contextual understanding, stability, and leaderboard consistency across all 18 experiments.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "Throughout the project, I explored a wide spectrum of models and feature combinations, completing 18 different submissions in the process. I started with simple baselines such as BOW with Naive Bayes and TF-IDF with Logistic Regression, which served as important anchors for understanding the task. Building on these, I tested **TF-IDF with LinearSVC**, which immediately outperformed earlier models and became the strongest classical baseline. To further push performance, I experimented with various feature concatenation schemes, blending TF-IDF, Word2Vec embeddings, and my set of hand-crafted statistical features. Although these hybrid representations improved performance moderately, they still could not exceed the linear TF-IDF baseline by a large margin.\n",
    "\n",
    "I also evaluated multiple Word2Vec-based pipelines, including Word2Vec + SVM, Word2Vec + XGBoost, and Word2Vec + LightGBM, though all of them underperformed due to the limitations of average-pooled dense embeddings. To incorporate phrase-level understanding, I trained a CNN classifier, which outperformed W2V models but still remained behind TF-IDF + SVM.\n",
    "\n",
    "As the project advanced, I gradually shifted toward modern deep learning approaches, fine-tuning BERT-base, RoBERTa-base, and DeBERTa-v3 (ran with 2 hours) under different freezing/voting ratios and epoch settings. Even with limited training time (1–2 epochs), Transformers achieved dramatic improvements. I then experimented with a hybrid approach, extracting embeddings from partially frozen Transformers and feeding them into LinearSVC, which significantly stabilized predictions compared to end-to-end fine-tuning. Variants such as DeBERTa + SVM (85% frozen) and DeBERTa + SVM with adjusted masking consistently yielded the highest performance, reaching around 0.70 on the public leaderboard. I also attempted soft voting ensembles that combined Transformer, SVM, and XGBoost predictions, though these did not surpass the strongest DeBERTa models.\n",
    "\n",
    "Also, I've learned how to fine-tune different models and test them by myself before submissions, especially with the 5-fold Cross Validation, which is highly representitive of the results. \n",
    "\n",
    "Overall, this iterative process: from classical baselines to deep hybrid models, allowed me to understand how each modeling family behaves on emotion classification and informed the selection of my final submission.\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "Several insights emerged during the model development process. First, I was surprised by **how strong TF-IDF combined with linear models remained**, even compared to more complex deep learning architectures. Emotional tweets tend to rely heavily on explicit lexical cues, such as sentiment-charged words and repeated punctuation, which TF-IDF captures very effectively. In contrast, Word2Vec embeddings diluted emotional signals because semantic similarity rarely aligns with emotional polarity; embedding-near words like “burning”, “fire”, and “lit” do not necessarily express the same sentiment. Additionally, average pooling compresses a sentence into a dense vector that often erases nuanced emotional patterns, making W2V-based models far less competitive.\n",
    "\n",
    "My experiments with deep learning highlighted how robust Transformer models are, even with limited fine-tuning. Despite training for only one or two epochs, models like BERT and RoBERTa reached 0.67–0.69 on the public leaderboard. However, I also observed that deeper fine-tuning sometimes caused overfitting, leading to performance drops on the private leaderboard. This led me to adopt hybrid approaches, extracting embeddings from DeBERTa or RoBERTa and training a separate LinearSVC classifier on top, which significantly improved the stability and generalization of the predictions. The consistency between public and private leaderboard rankings became a key indicator for model reliability, and models that overfit—such as CNNs or heavily fine-tuned Transformers—were quickly filtered out.\n",
    "\n",
    "Finally, while PAMI frequent pattern mining did not directly improve model scores, it offered valuable insights into how emotional meaning often arises from multi-word collocations rather than single tokens. Patterns like (“not”, “happy”) or (“feeling”, “down”) reinforced the importance of contextual modeling, which ultimately guided me toward adopting Transformer-based methods. Collectively, these insights shaped my modeling strategy and deepened my understanding of the relationships between feature representations, model families, and leaderboard behavior.\n",
    "\n",
    "I've written psuedo labeling code but unluckly didn't have time to test it. In the future, if I get time, I would like to dig more into hyperparameters of the models or other combination of voting method, to see if there is any other possibility to classify the emotions more accurately.\n",
    "\n",
    "---\n",
    "\n",
    "### Submission Records (Sorted by score: highest to lowest)\n",
    "![score1.png](./pics/score1.png)\n",
    "![score2.png](./pics/score2.png)\n",
    "![score3.png](./pics/score3.png)\n",
    "![score4.png](./pics/score4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "Dear TAs, since my python notebook for the competition is a bit large, so I put all my code in the file **\"DMLab2Kaggle.ipynb\"**.\n",
    "\n",
    "You can find all my code there, sorry for the incovenience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025-Lab2-Exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
