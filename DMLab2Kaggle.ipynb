{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9300e4b",
   "metadata": {},
   "source": [
    "# Data Mining Lab 2 Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098843df",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "(The links might not work well but the tabel indicates the correct structure of this notebook)\n",
    "\n",
    "## 1. Setup\n",
    "- [Import libraries and global settings](#import-libraries-and-global-settings)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Preparation\n",
    "- [Creating known and predict dataframes](#creating-known-and-predict-dataframes)\n",
    "- [Checking data characteristics](#checking-data-characteristics)\n",
    "- [Train-test split](#train-test-split)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Feature Engineering\n",
    "- [3.0 Feature Concat](#30-feature-concat)\n",
    "- [3.1 BOW](#31-bow)\n",
    "- [3.2 TF-IDF](#32-tf-idf)\n",
    "- [3.3 PAMI Feature Extraction](#33-pami-feature-extraction)\n",
    "\n",
    "Word2Vec ç³»åˆ—\n",
    "- [3.4.1 Word2Vec](#341-word2vec)\n",
    "- [3.4.2 Word2Vec + TF-IDF Weighted Mean](#342-word2vec-tf-idf-weighted-mean)\n",
    "- [3.4.3 TF-IDF + Word2Vec + Feature Concat](#343-tf-idf-word2vec-feature-concat)\n",
    "\n",
    "SVD ç³»åˆ—\n",
    "- [3.5.1 SVD](#341-svd)\n",
    "- [3.5.2 TF-IDF + Word2Vec + SVD + Feature Concat](#352-tf-idf-word2vec-svd-feature-concat)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Model Training\n",
    "- [4.1 Naive Bayes](#41-naive-bayes)\n",
    "- [4.2 Logistic Regression](#42-logistic-regression)\n",
    "- [4.3 LinearSVC](#43-linearsvc)\n",
    "- [4.4 XGBoost](#44-xgboost)\n",
    "- [4.5 LightGBM](#45-lightgbm)\n",
    "- [4.6 CNN](#46-cnn)\n",
    "- [4.7 BERT](#47-bert)\n",
    "- [4.8 RoBERTa](#48-roberta)\n",
    "\n",
    "---\n",
    "\n",
    "### Cross Validation\n",
    "- [5-Fold Cross Validation](#5-fold-cross-validation)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Final Submissions\n",
    "\n",
    "å‚³çµ± ML + Features\n",
    "- [5.1 BOW + NB](#51-bow--nb)\n",
    "- [5.2 TF-IDF + LR](#52-tf-idf--lr)\n",
    "- [5.3 TF-IDF + LinearSVC](#53-tf-idf--linearsvc)\n",
    "- [5.4 Word2Vec + LinearSVC](#54-word2vec--linearsvc)\n",
    "- [5.5 Word2Vec + XGBoost](#55-word2vec--xgboost)\n",
    "- [5.6 Word2Vec + LightGBM](#56-word2vec--lightgbm)\n",
    "- [5.7 TF-IDF + XGBoost](#57-tf-idf--xgboost)\n",
    "- [5.8 TF-IDF + LightGBM](#58-tf-idf--lightgbm)\n",
    "- [5.9 TF-IDF + Word2Vec + Feature Concat + LinearSVC](#59-tf-idf--word2vec--feature-concat--linearsvc)\n",
    "\n",
    "æ·±åº¦å­¸ç¿’ + Ensemble / Voting\n",
    "- [5.10 CNN](#510-cnn)\n",
    "- [5.11 BERT-base](#511-bert-base)\n",
    "- [5.12 RoBERTa](#512-roberta)\n",
    "- [5.13 Soft Voting (50% RoBERTa + 30% SBERT + 20% XGBoost)](#513-soft-voting-50-roberta--30-sbert--20-xgboost)\n",
    "- [5.14 DeBERTa](#514-deberta)\n",
    "- [5.15.1 Soft Voting (85% DeBERTa + 15% LinearSVC (TF-IDF))](#5151-soft-voting-85-deberta--15-linearsvc-tf-idf)\n",
    "- [5.15.2 Soft Voting (80% DeBERTa + 20% LinearSVC (TF-IDF))](#5152-soft-voting-80-deberta--20-linearsvc-tf-idf)\n",
    "- [5.16 Psuedo Labeling (with DeBERTa)](#516-psuedo-labeling-with-deberta)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ac2883",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e1b86",
   "metadata": {},
   "source": [
    "### Import libraries and global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19029316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omnl0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# --- æ ¸å¿ƒè³‡æ–™è™•ç†èˆ‡æ•¸å€¼é‹ç®— ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import emoji\n",
    "import re\n",
    "from scipy.sparse import hstack\n",
    "import os\n",
    "import PAMI\n",
    "from PAMI.frequentPatterns.fpgrowth import fpgrowth as FP_Growth\n",
    "\n",
    "# --- è‡ªç„¶èªè¨€è™•ç† ---\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# --- æ©Ÿå™¨å­¸ç¿’èˆ‡è©•ä¼° ---\n",
    "import torch\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, pairwise\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout\n",
    "from tensorflow.keras.layers import ReLU, Softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# --- æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ ---\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier, callback\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "\n",
    "# --- è³‡æ–™è¦–è¦ºåŒ– ---\n",
    "import plotly as py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- é€²éšåˆ†æï¼šé »ç¹æ¨¡å¼æ¢å‹˜èˆ‡é™ç¶­ ---\n",
    "import PAMI\n",
    "import umap\n",
    "\n",
    "# --- å¯¦é©—è¼”åŠ©å‡½æ•¸ ---\n",
    "import helpers.data_mining_helpers as dmh # è¼‰å…¥å¯¦é©—æä¾›çš„è¼”åŠ©å‡½æ•¸\n",
    "\n",
    "# NLTK ç›¸é—œè³‡æ–™ä¸‹è¼‰ï¼Œç”¨æ–¼æ–·è©\n",
    "nltk.download('punkt')\n",
    "\n",
    "# è®€å–å„csvæª”æ¡ˆ\n",
    "ident=pd.read_csv(\"data_identification.csv\")\n",
    "train_emotion=pd.read_csv(\"emotion.csv\")\n",
    "\n",
    "# å®šç¾©emotion\n",
    "emotions={\"anger\", \"disgust\", \"fear\", \"sadness\", \"surprise\", \"joy\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dad3a4c",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca724e6a",
   "metadata": {},
   "source": [
    "### Creating known and predict dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d11a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             id                                               text  emotion\n",
      "0      0x35663e  I bet there is an army of married couples who ...      joy\n",
      "1      0xc78afe                         This could only end badly.     fear\n",
      "2      0x90089c  My sister squeezed a lime in her milk when she...      joy\n",
      "3      0x2ffb63                                Thank you so muchâ¤ï¸      joy\n",
      "4      0x989146  Stinks because ive been in this program for a ...      joy\n",
      "...         ...                                                ...      ...\n",
      "47885  0xd740f2                  why is everybody seem sp serious?      joy\n",
      "47886  0x99267e  You can cross fuck off, its 10f all winter in ...    anger\n",
      "47887  0x4afbe1  Guilty Gear actually did that before with Guil...    anger\n",
      "47888  0xf5ba78                       One of my favorite episodes.      joy\n",
      "47889  0xb5a35a  Texans and Astros both shut out tonight. Houst...  sadness\n",
      "\n",
      "[47890 rows x 3 columns]\n",
      "             id                                               text\n",
      "0      0x61fc95  We got the ranch, loaded our guns and sat up t...\n",
      "4      0xaba820         and that got my head bobbing a little bit.\n",
      "5      0x66e44d                Same. Glad it's not just out store.\n",
      "6      0xc03cf5  Like always i will wait and see thanks for the...\n",
      "8      0x02f65a  There's a bit of room between \"not loving sub-...\n",
      "...         ...                                                ...\n",
      "64146  0x0f273c                We all do it sometimes don't worry.\n",
      "64150  0xfc4c5d  This New Year I visited more relatives than us...\n",
      "64157  0xb318a3  R u a dad or did ur dad leave u both have bad ...\n",
      "64168  0x8f758e  I got my first raspberry from a crowd surfer f...\n",
      "64170  0x3a9174  Pre-prepare direction plays hale and hearty si...\n",
      "\n",
      "[16281 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "raw_data=pd.read_json(\"final_posts.json\")\n",
    "\n",
    "df = pd.json_normalize(raw_data[\"root\"])\n",
    "df=df[[\"_source.post.post_id\",\"_source.post.text\"]] #select the columns that I want\n",
    "df=df.rename(columns={\n",
    "    \"_source.post.post_id\":\"id\",\n",
    "    \"_source.post.text\":\"text\"\n",
    "})\n",
    "\n",
    "df=df.merge(ident,on=\"id\",how=\"left\")\n",
    "known_df=df[df[\"split\"]==\"train\"].copy()\n",
    "predict_df=df[df[\"split\"]==\"test\"].copy()\n",
    "\n",
    "known_df=known_df.merge(train_emotion,on=\"id\",how=\"left\")\n",
    "known_df = known_df.drop(columns=[\"split\"])\n",
    "predict_df = predict_df.drop(columns=[\"split\"])\n",
    "print(known_df)\n",
    "print(predict_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d65913",
   "metadata": {},
   "source": [
    "### Checking data characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce8d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Anger percent=0.22330340363332637\n",
      "Disgust percent=0.02470244309876801\n",
      "Fear percent=0.041950302777197745\n",
      "Sadness percent=0.08197953643766966\n",
      "Surprise percent=0.13115472958864063\n",
      "Joy percent=0.4969095844643976\n"
     ]
    }
   ],
   "source": [
    "print(predict_df.isnull().sum().sum())\n",
    "print(known_df.isnull().sum().sum())\n",
    "print(predict_df.duplicated().sum())\n",
    "print(known_df.duplicated().sum())\n",
    "\n",
    "print(f\"Anger percent={len(known_df[known_df['emotion']=='anger'])/len(known_df)}\")\n",
    "print(f\"Disgust percent={len(known_df[known_df['emotion']=='disgust'])/len(known_df)}\")\n",
    "print(f\"Fear percent={len(known_df[known_df['emotion']=='fear'])/len(known_df)}\")\n",
    "print(f\"Sadness percent={len(known_df[known_df['emotion']=='sadness'])/len(known_df)}\")\n",
    "print(f\"Surprise percent={len(known_df[known_df['emotion']=='surprise'])/len(known_df)}\")\n",
    "print(f\"Joy percent={len(known_df[known_df['emotion']=='joy'])/len(known_df)}\")\n",
    "\n",
    "#ç™¼ç¾è³‡æ–™é›†åº¦ä¸å¹³è¡¡ï¼Œå¾ŒçºŒtrain modelæ™‚è¦ç‰¹åˆ¥æ³¨æ„ï¼ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7333a60",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b893873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=known_df[\"text\"]\n",
    "y=known_df[\"emotion\"]\n",
    "\n",
    "x_train_text, x_test_text, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ea1775",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fa96f2",
   "metadata": {},
   "source": [
    "### 3.0 Feature Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed5b0887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text_series):\n",
    "    features = pd.DataFrame()\n",
    "    #ç¢ºä¿è½‰ç‚ºå­—ä¸²ä¸¦è½‰å°å¯«ä»¥åˆ©è™•ç†\n",
    "    texts = text_series.astype(str)\n",
    "    texts_lower = texts.str.lower()\n",
    "    \n",
    "    features[\"char_len\"] = texts.apply(len)\n",
    "    features[\"word_len\"] = texts.apply(lambda x: len(x.split()))\n",
    "    \n",
    "    safe_len = features[\"char_len\"] + 1\n",
    "    safe_word_len = features[\"word_len\"] + 1 #é¿å…åˆ†æ¯ç‚º0\n",
    "    \n",
    "    features[\"excl_count\"] = texts.apply(lambda x: x.count(\"!\"))\n",
    "    features[\"ques_count\"] = texts.apply(lambda x: x.count(\"?\"))\n",
    "    features[\"upper_ratio\"] = texts.apply(lambda x: sum(1 for c in x if c.isupper())) / safe_len\n",
    "    negation_list = [\"not\", \"no\", \"never\", \"none\", \"n't\", \"nothing\", \"nowhere\"]\n",
    "    features[\"negation\"] = texts_lower.apply(lambda x: int(any(w in x.split() for w in negation_list))) #å¦å®šè©\n",
    "    features[\"emoji_count\"] = texts.apply(lambda x: sum(1 for c in x if c in emoji.EMOJI_DATA))\n",
    "    features[\"repeat_punct\"] = texts.apply(lambda x: len(re.findall(r'([!?.]){2,}', x))) #é€£çºŒé©šå˜†è™Ÿorå•è™Ÿ\n",
    "    features[\"ellipsis\"] = texts.apply(lambda x: x.count(\"...\")) # åˆªç¯€è™Ÿ\"...\"\n",
    "\n",
    "    features[\"elongated\"] = texts_lower.apply(lambda x: len(re.findall(r'(.)\\1{2,}', x))) #åŒä¸€å€‹å­—å…ƒé‡è¤‡é€£çºŒå‡ºç¾ 3 æ¬¡ä»¥ä¸Š\n",
    "    first_person = [\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\"]\n",
    "    features[\"first_pronoun\"] = texts_lower.apply(lambda x: sum(1 for w in x.split() if w in first_person))\n",
    "    second_person = [\"you\", \"your\", \"yours\", \"u\", \"ur\"]\n",
    "    features[\"second_pronoun\"] = texts_lower.apply(lambda x: sum(1 for w in x.split() if w in second_person))\n",
    "    features[\"mentions\"] = texts.apply(lambda x: x.count(\"@\")) #æ¨™è¨˜\n",
    "    features[\"hashtags\"] = texts.apply(lambda x: x.count(\"#\"))\n",
    "    def get_avg_word_len(x): #å¹³å‡å­—é•·\n",
    "        words = x.split()\n",
    "        if len(words) == 0: return 0\n",
    "        return sum(len(w) for w in words) / len(words)\n",
    "    features[\"avg_word_len\"] = texts.apply(get_avg_word_len)\n",
    "    features[\"digit_count\"] = texts.apply(lambda x: sum(c.isdigit() for c in x)) #æ•¸å­—\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3429ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_features=extract_features(x_train_text)\n",
    "x_test_features=extract_features(x_test_text)\n",
    "\n",
    "#Three different scalers\n",
    "#scaler=StandardScaler()\n",
    "scaler=RobustScaler()\n",
    "#scaler=MinMaxScaler() \n",
    "x_train_feat_scaled=scaler.fit_transform(x_train_features)\n",
    "x_test_feat_scaled=scaler.transform(x_test_features)\n",
    "\n",
    "x_train=hstack([x_train,x_train_feat_scaled]).tocsr()\n",
    "x_test=hstack([x_test,x_test_feat_scaled]).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d111b471",
   "metadata": {},
   "source": [
    "### 3.1 BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4b236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!' '#' '$' ... 'ğŸ§€' '\\U000fe334' '\\U000fe358ğŸ½\\\\nhave']\n",
      "['.' 'i' 'the' ... 'mj' 'mizzymaxx' 'misspoke']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['mrs'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer=CountVectorizer(tokenizer=nltk.word_tokenize)\n",
    "df_count=vectorizer.fit_transform(x_train_text)\n",
    "train_freq=np.asarray(df_count.sum(axis=0))[0]\n",
    "vocab=vectorizer.get_feature_names_out()\n",
    "print(vocab)\n",
    "\n",
    "n=len(vocab)\n",
    "descend_sorted=np.argsort(train_freq)[::-1]\n",
    "top4_idx=descend_sorted[:int(n*0.04)] #Create vectorizer and filter top and bottom words (Find stop words)\n",
    "\n",
    "ascend_sorted=np.argsort(train_freq)\n",
    "bottom1_idx=ascend_sorted[:int(n*0.01)]\n",
    "\n",
    "words_to_remove=np.concatenate([vocab[top4_idx],vocab[bottom1_idx]])\n",
    "print(words_to_remove)\n",
    "\n",
    "vectorizer_clean=CountVectorizer(\n",
    "    stop_words=words_to_remove.tolist(),\n",
    "    tokenizer=nltk.word_tokenize\n",
    ")\n",
    "\n",
    "x_train=vectorizer_clean.fit_transform(x_train_text)\n",
    "x_test=vectorizer_clean.transform(x_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be1207",
   "metadata": {},
   "source": [
    "### 3.2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb4ad1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (38312, 68319)\n"
     ]
    }
   ],
   "source": [
    "tfidfvect=TfidfVectorizer(\n",
    "    ngram_range=(1,2), #better than (1,1) or (2,2)\n",
    "    tokenizer=nltk.word_tokenize,\n",
    "    token_pattern=None,\n",
    "    min_df=2, #è‡³å°‘å‡ºç¾å¹¾æ¬¡\n",
    "    max_df=0.95, #æœ€å¤šå‡ºç¾åœ¨å¤šå°‘textè£¡é¢\n",
    "    #stop_words=words_to_remove.tolist()\n",
    "    )\n",
    "\n",
    "x_train_tfidf=tfidfvect.fit_transform(x_train_text)\n",
    "x_test_tfidf=tfidfvect.transform(x_test_text)\n",
    "\n",
    "print(\"x_train shape:\", x_train_tfidf.shape)\n",
    "\n",
    "x_train=x_train_tfidf\n",
    "x_test=x_test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ef773",
   "metadata": {},
   "source": [
    "### 3.3 PAMI Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ad145",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = []\n",
    "\n",
    "for text in x_train_text:\n",
    "    tokens = text.lower().split()\n",
    "    tokens = list(set(tokens)) \n",
    "    transactions.append(tokens)\n",
    "\n",
    "print(\"Example transaction:\", transactions[0])\n",
    "print(\"Total transactions:\", len(transactions))\n",
    "\n",
    "transaction_file = \"transactions.txt\"\n",
    "\n",
    "with open(transaction_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for items in transactions:\n",
    "        line = \" \".join(items)\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"Saved transactional data to:\", transaction_file)\n",
    "\n",
    "min_support = 0.03\n",
    "\n",
    "miner = FP_Growth(transaction_file, minSup=min_support)\n",
    "miner.mine()\n",
    "\n",
    "patterns = miner.getPatterns()\n",
    "\n",
    "print(\"Number of discovered patterns:\", len(patterns))\n",
    "\n",
    "display_limit = 10\n",
    "\n",
    "print(f\"\\nShowing top {display_limit} frequent patterns:\\n\")\n",
    "for i, (pattern, support) in enumerate(patterns.items()):\n",
    "    if i == display_limit:\n",
    "        break\n",
    "    print(f\"Pattern: {pattern}  |  Support: {support}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd59d84",
   "metadata": {},
   "source": [
    "### 3.4.1 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dc6f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = pd.concat([known_df[\"text\"], predict_df[\"text\"]]) \n",
    "all_tokens = [nltk.word_tokenize(t.lower()) for t in all_text]\n",
    "x_train_tokens=[nltk.word_tokenize(t.lower()) for t in x_train_text]\n",
    "x_test_tokens=[nltk.word_tokenize(t.lower()) for t in x_test_text]\n",
    "\n",
    "w2v=Word2Vec(\n",
    "    all_tokens,\n",
    "    vector_size=300, #300ç¶­\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    sg=1, #skip-gram\n",
    "    workers=4,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "def sentence_to_vector(tokens,model):\n",
    "    vectors = [model.wv[w] for w in tokens if w in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0) #ç”¨å¹³å‡ä¾†è¡¨ç¤ºå¥å­å‘é‡\n",
    "\n",
    "x_train_w2v = np.vstack([sentence_to_vector(tokens, w2v) for tokens in x_train_tokens])\n",
    "x_test_w2v = np.vstack([sentence_to_vector(tokens,w2v) for tokens in x_test_tokens])\n",
    "\n",
    "x_train=x_train_w2v\n",
    "x_test=x_test_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d57bc65",
   "metadata": {},
   "source": [
    "### 3.4.2 Word2Vec + Tf-IDF Weighted Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5b23828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38312/38312 [00:04<00:00, 9215.66it/s]\n",
      "Processing Test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9578/9578 [00:00<00:00, 10209.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (38312, 300)\n",
      "x_test shape: (9578, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word2id = tfidfvect.vocabulary_\n",
    "train_tfidf_matrix = tfidfvect.transform(x_train_text)\n",
    "test_tfidf_matrix = tfidfvect.transform(x_test_text)\n",
    "\n",
    "def get_weighted_sentence_vector(tokens, w2v_model, tfidf_row, word2id):\n",
    "    valid_vectors = []\n",
    "    weights = []\n",
    "    \n",
    "    unique_tokens = set(tokens)\n",
    "\n",
    "    coo = tfidf_row.tocoo()\n",
    "    tfidf_lookup = dict(zip(coo.col, coo.data))\n",
    "    \n",
    "    for word in unique_tokens:\n",
    "        \n",
    "        if word in w2v_model.wv:\n",
    "            vec = w2v_model.wv[word] #æª¢æŸ¥å­—æ˜¯å¦åœ¨W2Væ¨¡å‹ä¸­\n",
    "    \n",
    "            if word in word2id:\n",
    "                word_id = word2id[word]\n",
    "                weight = tfidf_lookup.get(word_id, 0.0) #æª¢æŸ¥å­—æ˜¯å¦åœ¨TF-IDFå­—å…¸ä¸­\n",
    "            else:\n",
    "                weight = 0.0\n",
    "            \n",
    "            if weight > 0:\n",
    "                valid_vectors.append(vec)\n",
    "                weights.append(weight)\n",
    "    \n",
    "    if len(valid_vectors) == 0:\n",
    "        return np.zeros(w2v_model.vector_size)\n",
    "    \n",
    "    valid_vectors = np.array(valid_vectors)\n",
    "    weights = np.array(weights)\n",
    "    \n",
    "    return np.average(valid_vectors, axis=0, weights=weights)\n",
    "\n",
    "x_train_list = []\n",
    "\n",
    "for i in tqdm(range(len(x_train_tokens)), desc=\"Processing Train\"):\n",
    "    tokens = x_train_tokens[i]\n",
    "    tfidf_row = train_tfidf_matrix[i]\n",
    "    vec = get_weighted_sentence_vector(tokens, w2v, tfidf_row, word2id)\n",
    "    x_train_list.append(vec)\n",
    "\n",
    "x_train = np.vstack(x_train_list)\n",
    "\n",
    "x_test_list = []\n",
    "for i in tqdm(range(len(x_test_tokens)), desc=\"Processing Test\"):\n",
    "    tokens = x_test_tokens[i]\n",
    "    tfidf_row = test_tfidf_matrix[i]\n",
    "    vec = get_weighted_sentence_vector(tokens, w2v, tfidf_row, word2id)\n",
    "    x_test_list.append(vec)\n",
    "\n",
    "x_test = np.vstack(x_test_list)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ad197",
   "metadata": {},
   "source": [
    "### 3.4.3 TF-IDF + Word2Vec + Feature Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8054d7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç‰¹å¾µå½¢ç‹€: (38312, 16)\n",
      "xtrainå½¢ç‹€ (x_train_final): (38312, 68635)\n",
      "xtestå½¢ç‹€ (x_test_final): (9578, 68635)\n"
     ]
    }
   ],
   "source": [
    "tfidfvect = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    tokenizer=nltk.word_tokenize,\n",
    "    token_pattern=None,\n",
    "    min_df=2,\n",
    "    max_df=0.95 \n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidfvect.fit_transform(x_train_text) \n",
    "X_test_tfidf = tfidfvect.transform(x_test_text)\n",
    "\n",
    "X_train_feat = extract_features(x_train_text)\n",
    "X_test_feat = extract_features(x_test_text)\n",
    "\n",
    "scaler = RobustScaler() #æ¨™æº–åŒ–ç‰¹å¾µï¼Œé¿å…æ•¸å€¼ç¶­åº¦ä¸ä¸€æ¨£\n",
    "\n",
    "X_train_feat_scaled = scaler.fit_transform(X_train_feat)\n",
    "X_test_feat_scaled = scaler.transform(X_test_feat)\n",
    "print(f\"ç‰¹å¾µå½¢ç‹€: {X_train_feat_scaled.shape}\")\n",
    "\n",
    "x_train = hstack([\n",
    "    X_train_tfidf,  #TF-IDF (Sparse)\n",
    "    x_train_w2v,    #W2V (Dense Numpy Array)\n",
    "    X_train_feat_scaled  #Handcrafted (Dense Numpy Array)\n",
    "]).tocsr()\n",
    "\n",
    "x_test = hstack([\n",
    "    X_test_tfidf,\n",
    "    x_test_w2v,\n",
    "    X_test_feat_scaled\n",
    "]).tocsr()\n",
    "\n",
    "print(f\"xtrainå½¢ç‹€ (x_train_final): {x_train.shape}\")\n",
    "print(f\"xtestå½¢ç‹€ (x_test_final): {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392652a",
   "metadata": {},
   "source": [
    "### 3.5.1 SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709b72e9",
   "metadata": {},
   "source": [
    "Utilize SVD to decade the dimension of TF-IDF from ten thousands to 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb7dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDFåŸå§‹ç¶­åº¦: (38312, 68319)\n",
      "SVDé™ç¶­å¾Œå½¢ç‹€: (38312, 300)\n",
      "è§£é‡‹è®Šç•°é‡: 0.2096\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    tokenizer=nltk.word_tokenize,\n",
    "    token_pattern=None,\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    #é€™è£¡ä¸è¨­max_features æˆ–è€…è¨­å¾ˆå¤§ï¼Œè®“SVDæ±ºå®šä¿ç•™ä»€éº¼\n",
    ")\n",
    "\n",
    "x_train_tfidf = tfidf.fit_transform(x_train_text)\n",
    "x_test_tfidf = tfidf.transform(x_test_text)\n",
    "print(f\"TF-IDFåŸå§‹ç¶­åº¦: {x_train_tfidf.shape}\")\n",
    "\n",
    "svd = TruncatedSVD(n_components=300, random_state=42) #SVDé™ç¶­ï¼Œ300ç¶­\n",
    "\n",
    "x_train_svd = svd.fit_transform(x_train_tfidf)\n",
    "x_test_svd = svd.transform(x_test_tfidf)\n",
    "\n",
    "scaler = StandardScaler() #æ¨™æº–åŒ–\n",
    "x_train_svd = scaler.fit_transform(x_train_svd)\n",
    "x_test_svd = scaler.transform(x_test_svd)\n",
    "\n",
    "print(f\"SVDé™ç¶­å¾Œå½¢ç‹€: {x_train_svd.shape}\")\n",
    "print(f\"è§£é‡‹è®Šç•°é‡: {svd.explained_variance_ratio_.sum():.4f}\") #é€™å€‹æ•¸å€¼è¶Šé«˜ä»£è¡¨ä¿ç•™çš„è³‡è¨Šè¶Šå¤šï¼Œé€šå¸¸0.3~0.5åœ¨æ–‡å­—è³‡æ–™ä¸Šå°±ç®—ä¸éŒ¯äº†\n",
    "\n",
    "x_train=x_train_svd\n",
    "x_test=x_test_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5358278c",
   "metadata": {},
   "source": [
    "### 3.5.2 TF-IDF + Word2Vec + SVD + Feature Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50c0b277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€çµ‚ç‰¹å¾µç¶­åº¦: (38312, 68935)\n"
     ]
    }
   ],
   "source": [
    "x_train = hstack([\n",
    "    x_train_tfidf,       \n",
    "    x_train_svd,         \n",
    "    x_train_w2v,         \n",
    "    x_train_feat_scaled \n",
    "]).tocsr() #è½‰ç‚ºCSRæ ¼å¼ä»¥å„ªåŒ–é‹ç®—é€Ÿåº¦\n",
    "\n",
    "x_test = hstack([\n",
    "    x_test_tfidf,\n",
    "    x_test_svd,\n",
    "    x_test_w2v,\n",
    "    x_test_feat_scaled\n",
    "]).tocsr()\n",
    "\n",
    "print(f\"æœ€çµ‚ç‰¹å¾µç¶­åº¦: {x_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bceb1bf",
   "metadata": {},
   "source": [
    "## 4.Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310515a5",
   "metadata": {},
   "source": [
    "### 4.1 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c16fb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy (NB): 0.688\n",
      "Testing accuracy (NB): 0.515\n",
      "\n",
      "Classification Report (NB):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.44      0.29      0.35      2139\n",
      "     disgust       0.17      0.01      0.02       237\n",
      "        fear       0.35      0.02      0.04       402\n",
      "         joy       0.54      0.90      0.67      4759\n",
      "     sadness       0.30      0.01      0.03       785\n",
      "    surprise       0.16      0.02      0.03      1256\n",
      "\n",
      "    accuracy                           0.51      9578\n",
      "   macro avg       0.32      0.21      0.19      9578\n",
      "weighted avg       0.43      0.51      0.42      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_model=MultinomialNB()\n",
    "NB_model.fit(x_train,y_train)\n",
    "\n",
    "NB_y_train_pred=NB_model.predict(x_train)\n",
    "NB_y_test_pred=NB_model.predict(x_test)\n",
    "\n",
    "NB_accuracy_train=accuracy_score(y_true=y_train,y_pred=NB_y_train_pred)\n",
    "NB_accuracy_test=accuracy_score(y_true=y_test,y_pred=NB_y_test_pred)\n",
    "\n",
    "print('Training accuracy (NB): {}'.format(round(NB_accuracy_train, 3)))\n",
    "print('Testing accuracy (NB): {}'.format(round(NB_accuracy_test, 3)))\n",
    "\n",
    "print(\"\\nClassification Report (NB):\")\n",
    "print(classification_report(y_true=y_test, y_pred=NB_y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc539dd2",
   "metadata": {},
   "source": [
    "### 4.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a04a942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy (LR): 0.467\n",
      "Testing accuracy (LR): 0.45\n",
      "\n",
      "Classification Report (LR):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.49      0.35      0.41      2139\n",
      "     disgust       0.06      0.32      0.10       237\n",
      "        fear       0.14      0.43      0.21       402\n",
      "         joy       0.82      0.50      0.62      4759\n",
      "     sadness       0.24      0.31      0.27       785\n",
      "    surprise       0.43      0.54      0.48      1256\n",
      "\n",
      "    accuracy                           0.45      9578\n",
      "   macro avg       0.36      0.41      0.35      9578\n",
      "weighted avg       0.60      0.45      0.50      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR_model=LogisticRegression(class_weight=\"balanced\") \n",
    "LR_model.fit(x_train,y_train)\n",
    "\n",
    "LR_y_train_pred=LR_model.predict(x_train)\n",
    "LR_y_test_pred=LR_model.predict(x_test)\n",
    "\n",
    "LR_accuracy_train=accuracy_score(y_true=y_train,y_pred=LR_y_train_pred)\n",
    "LR_accuracy_test=accuracy_score(y_true=y_test,y_pred=LR_y_test_pred)\n",
    "\n",
    "print('Training accuracy (LR): {}'.format(round(LR_accuracy_train, 3)))\n",
    "print('Testing accuracy (LR): {}'.format(round(LR_accuracy_test, 3)))\n",
    "\n",
    "print(\"\\nClassification Report (LR):\")\n",
    "print(classification_report(y_true=y_test, y_pred=LR_y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654e8eb",
   "metadata": {},
   "source": [
    "### 4.3 LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d5c4b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy (SVM): 0.547\n",
      "Testing accuracy (SVM): 0.532\n",
      "\n",
      "Classification Report (SVM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.50      0.44      0.47      2139\n",
      "     disgust       0.07      0.18      0.10       237\n",
      "        fear       0.16      0.38      0.23       402\n",
      "         joy       0.74      0.65      0.69      4759\n",
      "     sadness       0.35      0.25      0.29       785\n",
      "    surprise       0.47      0.51      0.49      1256\n",
      "\n",
      "    accuracy                           0.53      9578\n",
      "   macro avg       0.38      0.40      0.38      9578\n",
      "weighted avg       0.58      0.53      0.55      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_model=LinearSVC(C=0.15,class_weight=\"balanced\")\n",
    "SVM_model.fit(x_train,y_train)\n",
    "\n",
    "SVM_y_train_pred=SVM_model.predict(x_train)\n",
    "SVM_y_test_pred=SVM_model.predict(x_test)\n",
    "\n",
    "SVM_accuracy_train=accuracy_score(y_true=y_train,y_pred=SVM_y_train_pred)\n",
    "SVM_accuracy_test=accuracy_score(y_true=y_test,y_pred=SVM_y_test_pred)\n",
    "\n",
    "print('Training accuracy (SVM): {}'.format(round(SVM_accuracy_train, 3)))\n",
    "print('Testing accuracy (SVM): {}'.format(round(SVM_accuracy_test, 3)))\n",
    "\n",
    "print(\"\\nClassification Report (SVM):\")\n",
    "print(classification_report(y_true=y_test, y_pred=SVM_y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8169624c",
   "metadata": {},
   "source": [
    "### 4.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddebcba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨™ç±¤è½‰æ›ç¯„ä¾‹: 31193    sadness\n",
      "35246      anger\n",
      "23687      anger\n",
      "33582      anger\n",
      "7355         joy\n",
      "Name: emotion, dtype: object -> [4 0 0 0 3]\n",
      "é¡åˆ¥å°æ‡‰: [(0, 'anger'), (1, 'disgust'), (2, 'fear'), (3, 'joy'), (4, 'sadness'), (5, 'surprise')]\n",
      "[0]\tvalidation_0-mlogloss:1.78187\n",
      "[100]\tvalidation_0-mlogloss:1.47632\n",
      "[200]\tvalidation_0-mlogloss:1.38816\n",
      "[300]\tvalidation_0-mlogloss:1.33398\n",
      "[400]\tvalidation_0-mlogloss:1.29226\n",
      "[500]\tvalidation_0-mlogloss:1.26248\n",
      "[600]\tvalidation_0-mlogloss:1.23752\n",
      "[699]\tvalidation_0-mlogloss:1.21828\n",
      "Training accuracy (XGB): 0.836\n",
      "Testing accuracy (XGB): 0.534\n",
      "\n",
      "Classification Report (XGB):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.44      0.53      0.48      2139\n",
      "     disgust       0.17      0.07      0.10       237\n",
      "        fear       0.23      0.30      0.26       402\n",
      "         joy       0.74      0.63      0.68      4759\n",
      "     sadness       0.25      0.25      0.25       785\n",
      "    surprise       0.43      0.53      0.48      1256\n",
      "\n",
      "    accuracy                           0.53      9578\n",
      "   macro avg       0.38      0.38      0.37      9578\n",
      "weighted avg       0.56      0.53      0.54      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder() #ç·¨ç¢¼\n",
    "\n",
    "y_train_encoded = le.fit_transform(y_train) #å°‡æ–‡å­—æ¨™ç±¤è½‰æ›æˆæ•¸å­—\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "#æ¸¬è©¦æ˜¯å¦è½‰æ›æˆåŠŸ\n",
    "#print(\"æ¨™ç±¤è½‰æ›æ¸¬è©¦:\", y_train[:5], \"->\", y_train_encoded[:5])\n",
    "#print(\"é¡åˆ¥å°æ‡‰:\", list(enumerate(le.classes_)))\n",
    "\n",
    "train_sample_weights = compute_sample_weight(\n",
    "    class_weight='balanced',\n",
    "    y=y_train_encoded\n",
    ")\n",
    "\n",
    "XGB_model = XGBClassifier(\n",
    "    n_estimators=700, #1000æœƒoverfit\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5, #å¤ªæ·±ä¹Ÿæœƒoverfit\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.6,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.1,\n",
    "    reg_alpha=1,\n",
    "    objective=\"multi:softmax\",\n",
    "    num_class=6,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    n_jobs=-1,\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "XGB_model.fit(\n",
    "    x_train, \n",
    "    y_train_encoded, #é€™è£¡æ”¹æ”¾æ•¸å­—æ¨™ç±¤\n",
    "    sample_weight=train_sample_weights,\n",
    "    eval_set=[(x_test, y_test_encoded)],\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "XGB_y_train_pred_num = XGB_model.predict(x_train)\n",
    "XGB_y_test_pred_num = XGB_model.predict(x_test)\n",
    "\n",
    "XGB_y_train_pred = le.inverse_transform(XGB_y_train_pred_num) #ç”¨LabelEncoderæŠŠæ•¸å­—è½‰å›æ–‡å­—\n",
    "XGB_y_test_pred = le.inverse_transform(XGB_y_test_pred_num)\n",
    "\n",
    "XGB_accuracy_train = accuracy_score(y_true=y_train, y_pred=XGB_y_train_pred)\n",
    "XGB_accuracy_test = accuracy_score(y_true=y_test, y_pred=XGB_y_test_pred)\n",
    "\n",
    "print('Training accuracy (XGB): {}'.format(round(XGB_accuracy_train, 3)))\n",
    "print('Testing accuracy (XGB): {}'.format(round(XGB_accuracy_test, 3)))\n",
    "\n",
    "print(\"\\nClassification Report (XGB):\")\n",
    "print(classification_report(y_true=y_test, y_pred=XGB_y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818ae95",
   "metadata": {},
   "source": [
    "### 4.5 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59263b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's multi_logloss: 1.63805\n",
      "[100]\tvalid_0's multi_logloss: 1.57679\n",
      "[150]\tvalid_0's multi_logloss: 1.53999\n",
      "[200]\tvalid_0's multi_logloss: 1.51383\n",
      "[250]\tvalid_0's multi_logloss: 1.49322\n",
      "[300]\tvalid_0's multi_logloss: 1.47701\n",
      "[350]\tvalid_0's multi_logloss: 1.46272\n",
      "[400]\tvalid_0's multi_logloss: 1.45035\n",
      "[450]\tvalid_0's multi_logloss: 1.43933\n",
      "[500]\tvalid_0's multi_logloss: 1.42941\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's multi_logloss: 1.42941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy (LGBM): 0.538\n",
      "Testing accuracy (LGBM): 0.453\n",
      "\n",
      "Classification Report (LGBM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.44      0.40      0.42      2139\n",
      "     disgust       0.07      0.22      0.11       237\n",
      "        fear       0.15      0.44      0.22       402\n",
      "         joy       0.79      0.48      0.60      4759\n",
      "     sadness       0.24      0.32      0.27       785\n",
      "    surprise       0.42      0.55      0.48      1256\n",
      "\n",
      "    accuracy                           0.45      9578\n",
      "   macro avg       0.35      0.40      0.35      9578\n",
      "weighted avg       0.57      0.45      0.49      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LGBM_model=LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.02,\n",
    "    num_leaves=10,\n",
    "    class_weight='balanced', #LightGBMå…§å»ºæ”¯æ´ï¼Œä¸ç”¨ç®—sample_weight\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_alpha=1,\n",
    "    reg_lambda=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=-1,\n",
    "    max_depth=4,\n",
    "    min_child_samples=20\n",
    ")\n",
    "\n",
    "eval_set = [(x_test, y_test)]\n",
    "LGBM_model.fit(\n",
    "    x_train, \n",
    "    y_train,\n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"multi_logloss\",\n",
    "    callbacks=[\n",
    "        early_stopping(stopping_rounds=50),\n",
    "        log_evaluation(period=50)\n",
    "    ]\n",
    ")\n",
    "\n",
    "LGBM_y_train_pred=LGBM_model.predict(x_train)\n",
    "LGBM_y_test_pred=LGBM_model.predict(x_test)\n",
    "\n",
    "LGBM_accuracy_train=accuracy_score(y_true=y_train,y_pred=LGBM_y_train_pred)\n",
    "LGBM_accuracy_test=accuracy_score(y_true=y_test,y_pred=LGBM_y_test_pred)\n",
    "\n",
    "print('Training accuracy (LGBM): {}'.format(round(LGBM_accuracy_train, 3)))\n",
    "print('Testing accuracy (LGBM): {}'.format(round(LGBM_accuracy_test, 3)))\n",
    "\n",
    "print(\"\\nClassification Report (LGBM):\")\n",
    "print(classification_report(y_true=y_test, y_pred=LGBM_y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31bb9c5",
   "metadata": {},
   "source": [
    "### 4.6 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48085055",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000  #åªä¿ç•™æœ€å¸¸å‡ºç¾çš„2è¬å€‹å­—\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(x_train_text)\n",
    "\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train_text)\n",
    "x_test_seq = tokenizer.texts_to_sequences(x_test_text)\n",
    "\n",
    "x_train_pad = pad_sequences(x_train_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test_pad = pad_sequences(x_test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(f\"Train shape: {x_train_pad.shape}\")\n",
    "print(f\"Test shape: {x_test_pad.shape}\")\n",
    "\n",
    "le = LabelEncoder() #one-hot encoding\n",
    "y_train_le = le.fit_transform(y_train)\n",
    "y_test_le = le.transform(y_test)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "y_train_hot = to_categorical(y_train_le, num_classes=num_classes)\n",
    "y_test_hot = to_categorical(y_test_le, num_classes=num_classes)\n",
    "\n",
    "print(f\"Labels shape: {y_train_hot.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa32fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), name='Input')\n",
    "\n",
    "X = Embedding(input_dim=MAX_NUM_WORDS, \n",
    "              output_dim=EMBEDDING_DIM, \n",
    "              input_length=MAX_SEQUENCE_LENGTH,\n",
    "              name='Embedding')(model_input)\n",
    "\n",
    "X_Conv1 = Conv1D(filters=128, kernel_size=3, padding='valid', name='Conv1D_3gram')(X) # kernel_size=3: æ¯æ¬¡çœ‹3å€‹å­— (ç›¸ç•¶æ–¼Tri-gramï¼Œå­¸ç¿’ \"not happy yet\" é€™ç¨®çµæ§‹)\n",
    "H1 = ReLU()(X_Conv1) \n",
    "\n",
    "H2 = GlobalMaxPooling1D(name='GlobalMaxPool')(H1)\n",
    "\n",
    "H2_Drop = Dropout(0.5)(H2) #é˜²æ­¢æ¨¡å‹æ­»èƒŒoverfit\n",
    "H3_W1 = Dense(units=64, name='Dense_64')(H2_Drop)\n",
    "H3 = ReLU()(H3_W1)\n",
    "\n",
    "H3_W2 = Dense(units=num_classes, name='Output')(H3)\n",
    "model_output = Softmax()(H3_W2)\n",
    "\n",
    "model_cnn = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "model_cnn.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb19fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model_cnn.fit(\n",
    "    x_train_pad, y_train_hot,\n",
    "    validation_data=(x_test_pad, y_test_hot),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77deb3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model_cnn.predict(x_test_pad)\n",
    "y_pred_indices = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "y_pred_str = le.inverse_transform(y_pred_indices) #è½‰å›æ–‡å­—æ¨™ç±¤\n",
    "\n",
    "print(\"\\n=== CNN Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117bfce3",
   "metadata": {},
   "source": [
    "### 4.7 BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfa109e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA æ˜¯å¦å¯ç”¨: True\n",
      "é¡¯å¡æ•¸é‡: 1\n",
      "é¡¯å¡åç¨±: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "#ç¢ºèªæ˜¯å¦èƒ½å¤ ä½¿ç”¨GPUåŠ é€Ÿ\n",
    "import torch\n",
    "print(\"CUDA æ˜¯å¦å¯ç”¨:\", torch.cuda.is_available())\n",
    "print(\"é¡¯å¡æ•¸é‡:\", torch.cuda.device_count())\n",
    "print(\"é¡¯å¡åç¨±:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"ç„¡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf312031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨™ç±¤å°æ‡‰è¡¨: {'anger': 0, 'disgust': 1, 'fear': 2, 'joy': 3, 'sadness': 4, 'surprise': 5}\n",
      "è³‡æ–™æº–å‚™å®Œæˆï¼è¨“ç·´é›†: 38312 ç­†, æ¸¬è©¦é›†: 9578 ç­†\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(y) \n",
    "\n",
    "y_train_encoded = le.transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "print(\"æ¨™ç±¤å°æ‡‰è¡¨:\", dict(zip(le.classes_, range(len(le.classes_)))))\n",
    "\n",
    "train_texts = x_train_text.tolist()\n",
    "test_texts = x_test_text.tolist()\n",
    "train_labels = y_train_encoded.tolist()\n",
    "test_labels = y_test_encoded.tolist()\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Train: {len(train_texts)} ç­†, Test: {len(test_texts)} ç­†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb60aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128, #é€™å€‹åƒæ•¸æœƒå¤§å¹…å½±éŸ¿åŸ·è¡Œæ™‚é–“\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(label)\n",
    "        return item\n",
    "\n",
    "train_dataset = EmotionDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = EmotionDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    return {'accuracy': acc, 'f1_macro': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd9f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== é–‹å§‹è¨“ç·´ (ä½¿ç”¨ GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7185' max='7185' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7185/7185 27:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.864200</td>\n",
       "      <td>0.883813</td>\n",
       "      <td>0.680100</td>\n",
       "      <td>0.510368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.636500</td>\n",
       "      <td>0.931301</td>\n",
       "      <td>0.680831</td>\n",
       "      <td>0.518106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.405200</td>\n",
       "      <td>1.155406</td>\n",
       "      <td>0.673001</td>\n",
       "      <td>0.529059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== æ­£åœ¨ç”¢ç”Ÿæœ€çµ‚åˆ†é¡å ±å‘Š (Classification Report) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.60      0.63      0.61      2139\n",
      "     disgust       0.35      0.19      0.24       237\n",
      "        fear       0.51      0.55      0.53       402\n",
      "         joy       0.79      0.80      0.80      4759\n",
      "     sadness       0.46      0.43      0.44       785\n",
      "    surprise       0.56      0.53      0.55      1256\n",
      "\n",
      "    accuracy                           0.67      9578\n",
      "   macro avg       0.55      0.52      0.53      9578\n",
      "weighted avg       0.67      0.67      0.67      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(le.classes_)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert_results',\n",
    "    num_train_epochs=3, #è·‘3è¼ª\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\", #é¿å…tensor flowå›å ±éŒ¯èª¤\n",
    "    logging_dir=None,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\", #æ¯å€‹epochçµæŸé©—è­‰ä¸€æ¬¡\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True, #çµæŸæ™‚è¼‰å…¥åˆ†æ•¸æœ€å¥½çš„æ¨¡å‹\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(f\"é–‹å§‹train (ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}) ===\") #ç¢ºèªæœ‰ä½¿ç”¨GPUåŠ é€Ÿ\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(val_dataset)\n",
    "pred_indices = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "y_true_str = le.inverse_transform(test_labels)\n",
    "y_pred_str = le.inverse_transform(pred_indices)\n",
    "\n",
    "print(classification_report(y_true_str, y_pred_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f264ee0",
   "metadata": {},
   "source": [
    "### 4.8 roBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e38bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨™ç±¤å°æ‡‰è¡¨: {'anger': 0, 'disgust': 1, 'fear': 2, 'joy': 3, 'sadness': 4, 'surprise': 5}\n",
      "è³‡æ–™æº–å‚™å®Œæˆï¼è¨“ç·´é›†: 38312 ç­†, æ¸¬è©¦é›†: 9578 ç­†\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(y) \n",
    "\n",
    "y_train_encoded = le.transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "print(\"æ¨™ç±¤å°æ‡‰è¡¨:\", dict(zip(le.classes_, range(len(le.classes_)))))\n",
    "\n",
    "train_texts = x_train_text.tolist()\n",
    "test_texts = x_test_text.tolist()\n",
    "train_labels = y_train_encoded.tolist()\n",
    "test_labels = y_test_encoded.tolist()\n",
    "\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Train: {len(train_texts)} ç­†, Test: {len(test_texts)} ç­†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225b553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=64, #æ”¹æˆ64ï¼Œä¸ç„¶roBERTaæœƒè·‘å¾ˆä¹…\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        item = {key: val[0] for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = EmotionDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = EmotionDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    return {'accuracy': acc, 'f1_macro': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2460f43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== é–‹å§‹è¨“ç·´ (ä½¿ç”¨ GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4792' max='5990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4792/5990 21:43 < 05:26, 3.67 it/s, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.898808</td>\n",
       "      <td>0.680831</td>\n",
       "      <td>0.507054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.794000</td>\n",
       "      <td>0.880258</td>\n",
       "      <td>0.679474</td>\n",
       "      <td>0.507907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.694100</td>\n",
       "      <td>0.920300</td>\n",
       "      <td>0.685947</td>\n",
       "      <td>0.529845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.553000</td>\n",
       "      <td>0.984461</td>\n",
       "      <td>0.681875</td>\n",
       "      <td>0.527617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== æ­£åœ¨ç”¢ç”Ÿæœ€çµ‚åˆ†é¡å ±å‘Š (Classification Report) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.65      0.59      0.62      2139\n",
      "     disgust       0.33      0.16      0.22       237\n",
      "        fear       0.51      0.58      0.55       402\n",
      "         joy       0.78      0.85      0.81      4759\n",
      "     sadness       0.46      0.39      0.42       785\n",
      "    surprise       0.57      0.56      0.57      1256\n",
      "\n",
      "    accuracy                           0.69      9578\n",
      "   macro avg       0.55      0.52      0.53      9578\n",
      "weighted avg       0.67      0.69      0.68      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(le.classes_)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./roberta_results_optimized',\n",
    "    num_train_epochs=5, #æœ‰Early Stoppingæ‰€ä»¥å¯ä»¥å¢åŠ åˆ°5è¼ª\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    \n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\", #Cosineè¡°æ¸›\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    report_to=\"none\",\n",
    "    logging_dir=None,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    "    callbacks=[EarlyStoppingCallback(\n",
    "        early_stopping_patience=1, #Lossé€£çºŒ1è¼ªæ²’é€²æ­¥å°±åœæ­¢\n",
    "        early_stopping_threshold=0.0 #åªè¦æœ‰ä»»ä½•é€€æ­¥å°±åœæ­¢(åš´æ ¼)\n",
    "    )]\n",
    ")\n",
    "\n",
    "print(f\"é–‹å§‹è¨“ç·´ (ä½¿ç”¨ GPU: {torch.cuda.get_device_name(0)}) ===\") #ç¢ºèªæœ‰ä½¿ç”¨GPU\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(val_dataset)\n",
    "pred_indices = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "y_true_str = le.inverse_transform(test_labels)\n",
    "y_pred_str = le.inverse_transform(pred_indices)\n",
    "\n",
    "print(classification_report(y_true_str, y_pred_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe5d0d",
   "metadata": {},
   "source": [
    "### 5-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aac455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#é€™é‚Šçš„è¨­è¨ˆæ˜¯å¯ä»¥æ‹¿æ‰å‚™è¨»åˆ‡æ›ä¸åŒæ¨¡å‹æ¸¬è©¦5-fold CV\n",
    "\n",
    "#fold_model=MultinomialNB()\n",
    "\n",
    "#fold_model=LogisticRegression(class_weight=\"balanced\",max_iter=3000)\n",
    "\n",
    "fold_model=LinearSVC(C=0.15,class_weight=\"balanced\")\n",
    "\n",
    "# fold_model=XGBClassifier(\n",
    "#     n_estimators=700,\n",
    "#     learning_rate=0.05,\n",
    "#     max_depth=5,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.6,\n",
    "#     min_child_weight=3,\n",
    "#     gamma=0.1,\n",
    "#     reg_alpha=1,\n",
    "#     objective=\"multi:softmax\",\n",
    "#     num_class=6,\n",
    "#     eval_metric=\"mlogloss\",\n",
    "#     n_jobs=-1,\n",
    "#     tree_method=\"hist\",\n",
    "#     random_state=42,\n",
    "#     early_stopping_rounds=50\n",
    "# )\n",
    "\n",
    "# fold_model=LGBMClassifier(\n",
    "#     n_estimators=500,\n",
    "#     learning_rate=0.02,\n",
    "#     num_leaves=10,\n",
    "#     class_weight='balanced',\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.6,\n",
    "#     reg_alpha=1,\n",
    "#     reg_lambda=2,\n",
    "#     n_jobs=-1,\n",
    "#     random_state=42,\n",
    "#     verbose=-1,\n",
    "#     max_depth=4,\n",
    "#     min_child_samples=20\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9059681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 5-Fold Cross Validation =====\n",
      "\n",
      "--- Fold 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1 Score: 0.4880\n",
      "\n",
      "--- Fold 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 F1 Score: 0.4609\n",
      "\n",
      "--- Fold 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 F1 Score: 0.4773\n",
      "\n",
      "--- Fold 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 F1 Score: 0.4720\n",
      "\n",
      "--- Fold 5 ---\n",
      "Fold 5 F1 Score: 0.4805\n",
      "\n",
      "========================================\n",
      "5-Fold Average Macro F1 = 0.4757382433874876\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X = x_train\n",
    "y = y_train\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_scores = []\n",
    "\n",
    "print(\"===== 5-Fold Cross Validation =====\")\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"\\n--- Fold {fold+1} ---\")\n",
    "\n",
    "    X_tr, X_val = X[train_idx], X[valid_idx]\n",
    "    y_tr, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "    fold_model.fit(X_tr, y_tr)\n",
    "\n",
    "    val_preds = fold_model.predict(X_val)\n",
    "\n",
    "    fold_f1 = f1_score(y_val, val_preds, average=\"macro\")\n",
    "    fold_scores.append(fold_f1)\n",
    "\n",
    "    print(f\"Fold {fold+1} F1 Score: {fold_f1:.4f}\")\n",
    "\n",
    "print(\"\\n========================================\")\n",
    "print(\"5-Fold Average Macro F1 =\", np.mean(fold_scores))\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0762219",
   "metadata": {},
   "source": [
    "| Model         | Macro F1 | Accuracy | Notes         |\n",
    "| ------------- | -------- | -------- | ------------- |\n",
    "| NB            | 0.17     | 0.53     | baseline      |\n",
    "| LR (balanced) | 0.47     | 0.59     | balanced æ¬Šé‡æœ‰æ•ˆ |\n",
    "| SVM (C=0.1)   | 0.49     | 0.62     | æœ€ä½³å‚³çµ±æ¨¡å‹        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe3db0",
   "metadata": {},
   "source": [
    "## 5. Final Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e818cf",
   "metadata": {},
   "source": [
    "### 5.1 BOW + NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc64fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¼¸å‡ºå®Œæˆï¼šsubmission_bow_nb.csv\n"
     ]
    }
   ],
   "source": [
    "x_full=known_df[\"text\"]\n",
    "y_full=known_df[\"emotion\"]\n",
    "\n",
    "x_full_vectorized=vectorizer_clean.transform(x_full)\n",
    "x_test_final=vectorizer_clean.transform(predict_df[\"text\"])\n",
    "\n",
    "NB_model=MultinomialNB()\n",
    "NB_model.fit(x_full_vectorized,y_full)\n",
    "final_prediction=NB_model.predict(x_test_final)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": predict_df[\"id\"],\n",
    "    \"emotion\": final_prediction\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_bow_nb.csv\", index=False)\n",
    "print(\"è¼¸å‡ºå®Œæˆï¼šsubmission_bow_nb.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b1cb4a",
   "metadata": {},
   "source": [
    "### 5.2 TF-IDF + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d96a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¼¸å‡ºå®Œæˆï¼šsubmission_tfidf_lr.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "full_tfidfvect=TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    tokenizer=nltk.word_tokenize,\n",
    "    token_pattern=None,\n",
    "    min_df=2,\n",
    "    max_df=0.95)\n",
    "\n",
    "x_full=known_df[\"text\"]\n",
    "y_full=known_df[\"emotion\"]\n",
    "\n",
    "x_full_vectorized=full_tfidfvect.fit_transform(x_full)\n",
    "x_test_final=full_tfidfvect.transform(predict_df[\"text\"])\n",
    "\n",
    "LR_model=LogisticRegression(class_weight=\"balanced\")\n",
    "LR_model.fit(x_full_vectorized,y_full)\n",
    "final_prediction=LR_model.predict(x_test_final)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": predict_df[\"id\"],\n",
    "    \"emotion\": final_prediction\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_tfidf_lr.csv\", index=False)\n",
    "print(\"è¼¸å‡ºå®Œæˆï¼šsubmission_tfidf_lr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b929dc",
   "metadata": {},
   "source": [
    "### 5.3 TF-IDF + LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2083e8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¼¸å‡ºå®Œæˆï¼šsubmission_tfidf_svm.csv\n"
     ]
    }
   ],
   "source": [
    "full_tfidfvect=TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    tokenizer=nltk.word_tokenize,\n",
    "    token_pattern=None,\n",
    "    min_df=2,\n",
    "    max_df=0.95)\n",
    "\n",
    "x_full=known_df[\"text\"]\n",
    "y_full=known_df[\"emotion\"]\n",
    "\n",
    "x_full_vectorized=full_tfidfvect.fit_transform(x_full)\n",
    "x_test_final=full_tfidfvect.transform(predict_df[\"text\"])\n",
    "\n",
    "SVM_model=LinearSVC(C=0.1,class_weight=\"balanced\")\n",
    "SVM_model.fit(x_full_vectorized,y_full)\n",
    "final_prediction=SVM_model.predict(x_test_final)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": predict_df[\"id\"],\n",
    "    \"emotion\": final_prediction\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_tfidf_svm.csv\", index=False)\n",
    "print(\"è¼¸å‡ºå®Œæˆï¼šsubmission_tfidf_svm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a579857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¼¸å‡ºå®Œæˆï¼šsubmission_tfidf_featconcat_svm.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "full_tfidfvect = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    tokenizer=nltk.word_tokenize,\n",
    "    token_pattern=None,\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "x_full = known_df[\"text\"]\n",
    "y_full = known_df[\"emotion\"]\n",
    "\n",
    "X_full_tfidf = full_tfidfvect.fit_transform(x_full)\n",
    "X_test_tfidf = full_tfidfvect.transform(predict_df[\"text\"])\n",
    "\n",
    "X_full_feat = extract_features(x_full)\n",
    "X_test_feat = extract_features(predict_df[\"text\"])\n",
    "\n",
    "print(X_full_feat)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_full_feat_scaled = scaler.fit_transform(X_full_feat)\n",
    "X_test_feat_scaled  = scaler.transform(X_test_feat)\n",
    "\n",
    "X_full_final = hstack([X_full_tfidf, X_full_feat_scaled]).tocsr()\n",
    "X_test_final = hstack([X_test_tfidf, X_test_feat_scaled]).tocsr()\n",
    "\n",
    "SVM_model = LinearSVC(\n",
    "    C=0.15,\n",
    "    class_weight=\"balanced\",\n",
    ")\n",
    "\n",
    "SVM_model.fit(X_full_final, y_full)\n",
    "\n",
    "final_prediction = SVM_model.predict(X_test_final)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": predict_df[\"id\"],\n",
    "    \"emotion\": final_prediction\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_tfidf_featconcat_svm.csv\", index=False)\n",
    "print(\"è¼¸å‡ºå®Œæˆï¼šsubmission_tfidf_featconcat_svm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a020562",
   "metadata": {},
   "source": [
    "### 5.4 Word2Vec + LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6732abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec on full dataset...\n",
      "Vectorizing text...\n",
      "Training LinearSVC...\n",
      "è¼¸å‡ºå®Œæˆ: submission_w2v_svm.csv\n"
     ]
    }
   ],
   "source": [
    "all_text = pd.concat([known_df[\"text\"], predict_df[\"text\"]])\n",
    "all_tokens = [nltk.word_tokenize(t.lower()) for t in all_text]\n",
    "\n",
    "print(\"Training Word2Vec on full dataset\")\n",
    "w2v_model = Word2Vec(\n",
    "    all_tokens,\n",
    "    vector_size=300,\n",
    "    window=5,\n",
    "    min_count=2, #æ¿¾æ‰æ¥µå°‘å‡ºç¾çš„é›œè¨Š\n",
    "    sg=1,\n",
    "    workers=4,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "def sentence_to_vector(tokens, model):\n",
    "    vectors = [model.wv[w] for w in tokens if w in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "print(\"Vectorizing text...\")\n",
    "\n",
    "x_full_tokens = [nltk.word_tokenize(t.lower()) for t in known_df[\"text\"]]\n",
    "x_full_w2v = np.vstack([sentence_to_vector(tokens, w2v_model) for tokens in x_full_tokens])\n",
    "y_full = known_df[\"emotion\"]\n",
    "\n",
    "x_predict_tokens = [nltk.word_tokenize(t.lower()) for t in predict_df[\"text\"]]\n",
    "x_test_final_w2v = np.vstack([sentence_to_vector(tokens, w2v_model) for tokens in x_predict_tokens])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_full_w2v_scaled = scaler.fit_transform(x_full_w2v)\n",
    "x_test_final_w2v_scaled = scaler.transform(x_test_final_w2v)\n",
    "\n",
    "print(\"Training LinearSVC...\")\n",
    "\n",
    "SVM_model = LinearSVC(C=0.15, class_weight=\"balanced\", max_iter=2000) #ä½¿ç”¨ class_weight=\"balanced\"è§£æ±ºè³‡æ–™ä¸å¹³è¡¡\n",
    "SVM_model.fit(x_full_w2v_scaled, y_full)\n",
    "\n",
    "final_prediction = SVM_model.predict(x_test_final_w2v_scaled)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": predict_df[\"id\"],\n",
    "    \"emotion\": final_prediction\n",
    "})\n",
    "submission.to_csv(\"submission_w2v_svm.csv\", index=False)\n",
    "print(\"è¼¸å‡ºå®Œæˆ: submission_w2v_svm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed129be",
   "metadata": {},
   "source": [
    "### 5.5 Word2Vec + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ead63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost...\n",
      "Predicting...\n",
      "è¼¸å‡ºå®Œæˆ: submission_w2v_xgboost.csv\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_full_encoded = le.fit_transform(y_full)\n",
    "\n",
    "sample_weights = compute_sample_weight( #è§£æ±ºè³‡æ–™ä¸å¹³è¡¡\n",
    "    class_weight='balanced',\n",
    "    y=y_full_encoded\n",
    ")\n",
    "\n",
    "XGB_model = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.1,\n",
    "    reg_alpha=0.1,\n",
    "    objective=\"multi:softmax\",\n",
    "    num_class=6,\n",
    "    n_jobs=-1,\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "XGB_model.fit(\n",
    "    x_full_w2v,\n",
    "    y_full_encoded,\n",
    "    sample_weight=sample_weights,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Predicting\")\n",
    "pred_encoded = XGB_model.predict(x_test_final_w2v)\n",
    "final_prediction = le.inverse_transform(pred_encoded)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": predict_df[\"id\"],\n",
    "    \"emotion\": final_prediction\n",
    "})\n",
    "submission.to_csv(\"submission_w2v_xgboost.csv\", index=False)\n",
    "print(\"è¼¸å‡ºå®Œæˆ: submission_w2v_xgboost.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcf12c7",
   "metadata": {},
   "source": [
    "### 5.6 Word2Vec + LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d313c778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM...\n",
      "Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¼¸å‡ºå®Œæˆ: submission_w2v_lightgbm.csv\n"
     ]
    }
   ],
   "source": [
    "#é€™è£¡æ²¿ç”¨5.5çš„leå’Œy_full_encoded\n",
    "\n",
    "LGBM_model = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    class_weight='balanced',\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "LGBM_model.fit(x_full_w2v, y_full) # LGBMçš„sklearn APIå¯ä»¥ç›´æ¥åƒæ–‡å­—label\n",
    "\n",
    "print(\"Predicting\")\n",
    "final_prediction = LGBM_model.predict(x_test_final_w2v)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": predict_df[\"id\"],\n",
    "    \"emotion\": final_prediction\n",
    "})\n",
    "submission.to_csv(\"submission_w2v_lightgbm.csv\", index=False)\n",
    "print(\"è¼¸å‡ºå®Œæˆ: submission_w2v_lightgbm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb9ec6",
   "metadata": {},
   "source": [
    "### 5.7 TF-IDF + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc106de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing text with TF-IDF...\n",
      "Training XGBoost...\n",
      "Predicting...\n",
      "è¼¸å‡ºå®Œæˆ: submission_tfidf_xgboost.csv\n"
     ]
    }
   ],
   "source": [
    "full_tfidfvect = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    tokenizer=nltk.word_tokenize,\n",
    "    token_pattern=None,\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "x_full = known_df[\"text\"]\n",
    "y_full = known_df[\"emotion\"]\n",
    "\n",
    "x_full_tfidf = full_tfidfvect.fit_transform(x_full)\n",
    "x_test_final_tfidf = full_tfidfvect.transform(predict_df[\"text\"])\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_full_encoded = le.fit_transform(y_full)\n",
    "\n",
    "sample_weights = compute_sample_weight( #è§£æ±ºè³‡æ–™ä¸å¹³è¡¡\n",
    "    class_weight='balanced',\n",
    "    y=y_full_encoded\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost\")\n",
    "XGB_model = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.6, # TF-IDFç¶­åº¦é«˜ï¼Œå› æ­¤é™ä½ä¸€é»é€™å€‹åƒæ•¸\n",
    "    min_child_weight=3,\n",
    "    gamma=0.1,\n",
    "    reg_alpha=1,\n",
    "    objective=\"multi:softmax\",\n",
    "    num_class=6,\n",
    "    n_jobs=-1,\n",
    "    tree_method=\"hist\", #åŠ é€ŸsparseçŸ©é™£é‹ç®—\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "XGB_model.fit(\n",
    "    x_full_tfidf, \n",
    "    y_full_encoded,\n",
    "    sample_weight=sample_weights,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Predicting\")\n",
    "pred_encoded = XGB_model.predict(x_test_final_tfidf)\n",
    "final_prediction = le.inverse_transform(pred_encoded)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": predict_df[\"id\"],\n",
    "    \"emotion\": final_prediction\n",
    "})\n",
    "submission.to_csv(\"submission_tfidf_xgboost.csv\", index=False)\n",
    "print(\"è¼¸å‡ºå®Œæˆ: submission_tfidf_xgboost.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8841ac",
   "metadata": {},
   "source": [
    "### 5.8 TF-IDF + LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9957330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing text with TF-IDF...\n",
      "Training LightGBM...\n",
      "Predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¼¸å‡ºå®Œæˆ: submission_tfidf_lightgbm.csv\n"
     ]
    }
   ],
   "source": [
    "full_tfidfvect = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    tokenizer=nltk.word_tokenize,\n",
    "    token_pattern=None,\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "x_full = known_df[\"text\"]\n",
    "y_full = known_df[\"emotion\"]\n",
    "\n",
    "x_full_tfidf = full_tfidfvect.fit_transform(x_full)\n",
    "x_test_final_tfidf = full_tfidfvect.transform(predict_df[\"text\"])\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_full_encoded = le.fit_transform(y_full)\n",
    "\n",
    "print(\"Training LightGBM\")\n",
    "LGBM_model = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    class_weight='balanced',\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_alpha=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "LGBM_model.fit(x_full_tfidf, y_full_encoded)\n",
    "\n",
    "print(\"Predicting\")\n",
    "pred_encoded = LGBM_model.predict(x_test_final_tfidf)\n",
    "final_prediction = le.inverse_transform(pred_encoded)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": predict_df[\"id\"],\n",
    "    \"emotion\": final_prediction\n",
    "})\n",
    "submission.to_csv(\"submission_tfidf_lightgbm.csv\", index=False)\n",
    "print(\"è¼¸å‡ºå®Œæˆ: submission_tfidf_lightgbm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d02bf",
   "metadata": {},
   "source": [
    "### 5.9 TF-IDF + Word2Vec + Feature Concat + LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f1e168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. æ­£åœ¨æº–å‚™è³‡æ–™...\n",
      "2. æ­£åœ¨ç”Ÿæˆ TF-IDF ç‰¹å¾µ...\n",
      "TF-IDF Shape: (47890, 82438)\n",
      "3. æ­£åœ¨ç”Ÿæˆ Word2Vec ç‰¹å¾µ...\n",
      "4. æ­£åœ¨ç”Ÿæˆæ‰‹å·¥ç‰¹å¾µ...\n",
      "5. æ­£åœ¨åˆä½µæ‰€æœ‰ç‰¹å¾µ...\n",
      "æœ€çµ‚ç‰¹å¾µç¶­åº¦: (47890, 82744)\n",
      "6. æ­£åœ¨è¨“ç·´ LinearSVC...\n",
      "7. æ­£åœ¨é æ¸¬ä¸¦è¼¸å‡º...\n",
      "è¼¸å‡ºå®Œæˆï¼æª”æ¡ˆå·²å„²å­˜ç‚º: submission_tfidf_w2v_feat_svm.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x_full_text = known_df['text']\n",
    "y_full_text = known_df['emotion']\n",
    "x_predict_text = predict_df['text']\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_full_encoded = le.fit_transform(y_full_text)\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    tokenizer=nltk.word_tokenize,\n",
    "    token_pattern=None,\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "x_full_tfidf = tfidf.fit_transform(x_full_text)\n",
    "x_predict_tfidf = tfidf.transform(x_predict_text)\n",
    "print(f\"TF-IDF Shape: {x_full_tfidf.shape}\")\n",
    "\n",
    "all_text = pd.concat([x_full_text, x_predict_text])\n",
    "all_tokens = [nltk.word_tokenize(t.lower()) for t in all_text]\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    all_tokens,\n",
    "    vector_size=300,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    sg=1, # Skip-gram\n",
    "    workers=4,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "def sentence_to_vector(text, model):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    vectors = [model.wv[w] for w in tokens if w in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "x_full_w2v = np.vstack([sentence_to_vector(t, w2v_model) for t in x_full_text])\n",
    "x_predict_w2v = np.vstack([sentence_to_vector(t, w2v_model) for t in x_predict_text])\n",
    "\n",
    "scaler_w2v = MinMaxScaler()\n",
    "x_full_w2v_scaled = scaler_w2v.fit_transform(x_full_w2v)\n",
    "x_predict_w2v_scaled = scaler_w2v.transform(x_predict_w2v)\n",
    "\n",
    "x_full_feat = extract_features(x_full_text)\n",
    "x_predict_feat = extract_features(x_predict_text)\n",
    "\n",
    "scaler_feat = RobustScaler() #æ¨™æº–åŒ–\n",
    "x_full_feat_scaled = scaler_feat.fit_transform(x_full_feat)\n",
    "x_predict_feat_scaled = scaler_feat.transform(x_predict_feat)\n",
    "\n",
    "x_train_final = hstack([x_full_tfidf, x_full_w2v_scaled, x_full_feat_scaled]).tocsr() #åˆä½µ\\\n",
    "x_test_final = hstack([x_predict_tfidf, x_predict_w2v_scaled, x_predict_feat_scaled]).tocsr()\n",
    "\n",
    "print(f\"æœ€çµ‚ç‰¹å¾µç¶­åº¦: {x_train_final.shape}\")\n",
    "\n",
    "svm_model = LinearSVC(\n",
    "    C=0.15, \n",
    "    class_weight=\"balanced\", \n",
    "    max_iter=5000, #è¨­å¤§ä¸€é»ç¢ºä¿æ”¶æ–‚\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svm_model.fit(x_train_final, y_full_encoded)\n",
    "\n",
    "predictions = svm_model.predict(x_test_final)\n",
    "final_labels = le.inverse_transform(predictions)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": predict_df[\"id\"],\n",
    "    \"emotion\": final_labels\n",
    "})\n",
    "\n",
    "filename = \"submission_tfidf_w2v_feat_svm.csv\"\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"è¼¸å‡ºå®Œæˆï¼æª”æ¡ˆå·²å„²å­˜ç‚º: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7823161c",
   "metadata": {},
   "source": [
    "### 5.10 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48436f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. æ­£åœ¨æº–å‚™å…¨é‡è³‡æ–™...\n",
      "2. æ­£åœ¨é€²è¡Œæ–‡å­—åºåˆ—åŒ–...\n",
      "è¨“ç·´é›†çŸ©é™£å½¢ç‹€: (47890, 128)\n",
      "é æ¸¬é›†çŸ©é™£å½¢ç‹€: (16281, 128)\n",
      "3. è™•ç†æ¨™ç±¤ç·¨ç¢¼...\n",
      "æ¨™ç±¤å°æ‡‰: {'anger': 0, 'disgust': 1, 'fear': 2, 'joy': 3, 'sadness': 4, 'surprise': 5}\n",
      "4. å»ºæ§‹ CNN æ¨¡å‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ Input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,000,000</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Conv1D (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">115,328</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ GlobalMaxPool                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Dense_Hidden (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Softmax</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ Input (\u001b[38;5;33mInputLayer\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m300\u001b[0m)       â”‚     \u001b[38;5;34m6,000,000\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Conv1D (\u001b[38;5;33mConv1D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚       \u001b[38;5;34m115,328\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu (\u001b[38;5;33mReLU\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ GlobalMaxPool                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Dense_Hidden (\u001b[38;5;33mDense\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_1 (\u001b[38;5;33mReLU\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Output (\u001b[38;5;33mDense\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              â”‚           \u001b[38;5;34m390\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ softmax (\u001b[38;5;33mSoftmax\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,123,974</span> (23.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,123,974\u001b[0m (23.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,123,974</span> (23.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,123,974\u001b[0m (23.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. é–‹å§‹å…¨é‡è¨“ç·´...\n",
      "Epoch 1/15\n",
      "\u001b[1m711/711\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.4966 - loss: 1.4076 - val_accuracy: 0.4864 - val_loss: 1.3374\n",
      "Epoch 2/15\n",
      "\u001b[1m711/711\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.5288 - loss: 1.2521 - val_accuracy: 0.5541 - val_loss: 1.2135\n",
      "Epoch 3/15\n",
      "\u001b[1m711/711\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.5891 - loss: 1.1382 - val_accuracy: 0.5816 - val_loss: 1.1457\n",
      "Epoch 4/15\n",
      "\u001b[1m711/711\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.6291 - loss: 1.0487 - val_accuracy: 0.5954 - val_loss: 1.1027\n",
      "Epoch 5/15\n",
      "\u001b[1m711/711\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.6587 - loss: 0.9737 - val_accuracy: 0.6100 - val_loss: 1.0789\n",
      "Epoch 6/15\n",
      "\u001b[1m711/711\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.6866 - loss: 0.9048 - val_accuracy: 0.6205 - val_loss: 1.0677\n",
      "Epoch 7/15\n",
      "\u001b[1m711/711\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 54ms/step - accuracy: 0.7120 - loss: 0.8383 - val_accuracy: 0.6213 - val_loss: 1.0738\n",
      "Epoch 8/15\n",
      "\u001b[1m711/711\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 56ms/step - accuracy: 0.7321 - loss: 0.7817 - val_accuracy: 0.6246 - val_loss: 1.0942\n",
      "Epoch 9/15\n",
      "\u001b[1m711/711\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 55ms/step - accuracy: 0.7545 - loss: 0.7261 - val_accuracy: 0.6217 - val_loss: 1.1225\n",
      "\n",
      "6. æ­£åœ¨é æ¸¬ä¸¦è¼¸å‡ºçµæœ...\n",
      "\u001b[1m509/509\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step\n",
      "è¼¸å‡ºå®Œæˆï¼æª”æ¡ˆå·²å„²å­˜ç‚º: submission_cnn_full.csv\n"
     ]
    }
   ],
   "source": [
    "MAX_NUM_WORDS = 20000    # å­—å…¸åªä¿ç•™æœ€å¸¸å‡ºç¾çš„2è¬å­—\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "EMBEDDING_DIM = 300\n",
    "EPOCHS = 15 #è¨­å®šé«˜ä¸€é»ç„¶å¾Œearly stop\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "x_train_full = known_df['text'].astype(str).tolist()\n",
    "y_train_full = known_df['emotion'].tolist()\n",
    "\n",
    "x_predict_text = predict_df['text'].astype(str).tolist()\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(x_train_full)\n",
    "\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train_full)\n",
    "x_predict_seq = tokenizer.texts_to_sequences(x_predict_text)\n",
    "\n",
    "x_train_pad = pad_sequences(x_train_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_predict_pad = pad_sequences(x_predict_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(f\"train matrixå½¢ç‹€: {x_train_pad.shape}\")\n",
    "print(f\"test matrixå½¢ç‹€: {x_predict_pad.shape}\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_le = le.fit_transform(y_train_full)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "y_train_hot = to_categorical(y_train_le, num_classes=num_classes)\n",
    "\n",
    "print(f\"æ¨™ç±¤å°æ‡‰: {dict(zip(le.classes_, range(num_classes)))}\")\n",
    "\n",
    "model_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), name='Input')\n",
    "\n",
    "X = Embedding(input_dim=MAX_NUM_WORDS, \n",
    "              output_dim=EMBEDDING_DIM, \n",
    "              input_length=MAX_SEQUENCE_LENGTH,\n",
    "              name='Embedding')(model_input)\n",
    "\n",
    "X_Conv1 = Conv1D(filters=128, kernel_size=3, padding='valid', name='Conv1D')(X)\n",
    "H1 = ReLU()(X_Conv1)\n",
    "\n",
    "H2 = GlobalMaxPooling1D(name='GlobalMaxPool')(H1)\n",
    "\n",
    "H2_Drop = Dropout(0.5)(H2) #50%Dropout é˜²æ­¢overfit\n",
    "H3_W1 = Dense(units=64, name='Dense_Hidden')(H2_Drop)\n",
    "H3 = ReLU()(H3_W1)\n",
    "\n",
    "H3_W2 = Dense(units=num_classes, name='Output')(H3)\n",
    "model_output = Softmax()(H3_W2)\n",
    "\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_pad, y_train_hot,\n",
    "    validation_split=0.05, #åˆ‡5%åšå…§éƒ¨é©—è­‰\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_prob = model.predict(x_predict_pad)\n",
    "y_pred_indices = np.argmax(y_pred_prob, axis=1)\n",
    "final_prediction = le.inverse_transform(y_pred_indices)\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": predict_df[\"id\"],\n",
    "    \"emotion\": final_prediction\n",
    "})\n",
    "\n",
    "filename = \"submission_cnn_full.csv\"\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"è¼¸å‡ºå®Œæˆï¼æª”æ¡ˆå·²å„²å­˜ç‚º: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10175e22",
   "metadata": {},
   "source": [
    "### 5.11 BERT-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa9f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨æº–å‚™å…¨é‡è³‡æ–™...\n",
      "æ¨™ç±¤å°æ‡‰: {'anger': 0, 'disgust': 1, 'fear': 2, 'joy': 3, 'sadness': 4, 'surprise': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== é–‹å§‹å…¨é‡è¨“ç·´ (2 Epochs, Fixed) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2994' max='2994' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2994/2994 12:03, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.915200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.880300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.753200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.742100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== æ­£åœ¨å° Kaggle æ¸¬è©¦é›†é€²è¡Œé æ¸¬ ===\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¼¸å‡ºå®Œæˆï¼æª”æ¡ˆå·²å„²å­˜ç‚º: submission_bert_fixed_mask_epoch2.csv\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_full_encoded = le.fit_transform(known_df['emotion'])\n",
    "print(\"æ¨™ç±¤å°æ‡‰:\", dict(zip(le.classes_, range(len(le.classes_)))))\n",
    "\n",
    "train_texts = known_df['text'].tolist()\n",
    "train_labels = y_full_encoded.tolist()\n",
    "\n",
    "predict_texts = predict_df['text'].tolist()\n",
    "predict_dummy_labels = [0] * len(predict_texts) \n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=64,\n",
    "            \n",
    "            return_attention_mask=True,\n",
    "            \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        item = {key: val[0] for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "full_train_dataset = EmotionDataset(train_texts, train_labels, tokenizer)\n",
    "predict_dataset = EmotionDataset(predict_texts, predict_dummy_labels, tokenizer)\n",
    "\n",
    "num_labels = len(le.classes_)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert_full_submission_v2',\n",
    "    \n",
    "    num_train_epochs=2,              \n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    logging_dir=None,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=full_train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(predict_dataset)\n",
    "pred_indices = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "final_prediction = le.inverse_transform(pred_indices)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": predict_df[\"id\"],\n",
    "    \"emotion\": final_prediction\n",
    "})\n",
    "\n",
    "filename = \"submission_bert_fixed_mask_epoch2.csv\"\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"è¼¸å‡ºå®Œæˆï¼æª”æ¡ˆå·²å„²å­˜ç‚º: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a363915",
   "metadata": {},
   "source": [
    "### 5.12 roBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨æº–å‚™å…¨é‡è³‡æ–™...\n",
      "æ¨™ç±¤å°æ‡‰: {'anger': 0, 'disgust': 1, 'fear': 2, 'joy': 3, 'sadness': 4, 'surprise': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== é–‹å§‹å…¨é‡è¨“ç·´ (2 Epochs, Fixed) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2994' max='2994' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2994/2994 19:11, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.154100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.923100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.890700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.784400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.770700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== æ­£åœ¨å° Kaggle æ¸¬è©¦é›†é€²è¡Œé æ¸¬ ===\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¼¸å‡ºå®Œæˆï¼æª”æ¡ˆå·²å„²å­˜ç‚º: submission_roberta_2epochs.csv\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_full_encoded = le.fit_transform(known_df['emotion'])\n",
    "print(\"æ¨™ç±¤å°æ‡‰:\", dict(zip(le.classes_, range(len(le.classes_)))))\n",
    "\n",
    "train_texts = known_df['text'].tolist()\n",
    "train_labels = y_full_encoded.tolist()\n",
    "\n",
    "predict_texts = predict_df['text'].tolist()\n",
    "predict_dummy_labels = [0] * len(predict_texts) \n",
    "\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "            \n",
    "            return_attention_mask=True,\n",
    "            \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        item = {key: val[0] for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "full_train_dataset = EmotionDataset(train_texts, train_labels, tokenizer)\n",
    "predict_dataset = EmotionDataset(predict_texts, predict_dummy_labels, tokenizer)\n",
    "\n",
    "num_labels = len(le.classes_)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert_full_submission_v2',\n",
    "    \n",
    "    num_train_epochs=2,              \n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    logging_dir=None,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=full_train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "predictions = trainer.predict(predict_dataset)\n",
    "pred_indices = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "final_prediction = le.inverse_transform(pred_indices)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": predict_df[\"id\"],\n",
    "    \"emotion\": final_prediction\n",
    "})\n",
    "\n",
    "filename = \"submission_roberta_2epochs.csv\"\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"è¼¸å‡ºå®Œæˆï¼æª”æ¡ˆå·²å„²å­˜ç‚º: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30024d45",
   "metadata": {},
   "source": [
    "### 5.13 Soft Voting (50% roBERTa + 30% SBERT + 20% XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0f1c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 0. è³‡æ–™æº–å‚™ ===\n",
      "æ¨™ç±¤å°æ‡‰: {'anger': 0, 'disgust': 1, 'fear': 2, 'joy': 3, 'sadness': 4, 'surprise': 5}\n",
      "\n",
      "=== 1. è¨“ç·´ RoBERTa-base (ä¿®æ­£ç‰ˆ: Seed + Weight Decay) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2994' max='2994' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2994/2994 12:58, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.912100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.794700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.776300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ç”Ÿæˆ RoBERTa é æ¸¬æ©Ÿç‡...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 2. è¨“ç·´ SBERT (mpnet) + LR ===\n",
      "æ­£åœ¨ä¸‹è¼‰ä¸¦è¼‰å…¥ all-mpnet-base-v2 (æ›´æº–ç¢º)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb26dd3037b24853b85ec03ea6297b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\omnl0\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298d3b6f799f4b51b556913fb8b40c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbfd280cd1214ea4b7399f389cc82508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b9c983b10b4be3aac2bdfe8f848054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c389fc883402456b820727860d3d1f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7200949937514693962404389d21a06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "affddc87aee84d879c252dfc303e7423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e0e93566cc488498446ef3d647a92c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80123f52e56e4fb4acff0d17bc57146c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ae02d393a94bb6904bc82855ca335c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d8ee8396384365b4b952ea2c3a526a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT: Encoding Train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256effcdba84430f9c7edf93fc95db24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT: Encoding Predict...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d23b775a774880b3e55d43248b6b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 3. è¨“ç·´ TF-IDF (Default Tokenizer) + XGBoost ===\n",
      "\n",
      "=== 4. åŸ·è¡Œ Soft Voting èåˆ ===\n",
      "æœ€çµ‚ç‰ˆè¼¸å‡ºå®Œæˆï¼æª”æ¡ˆå·²å„²å­˜ç‚º: submission_soft_voting_optimized.csv\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_full = le.fit_transform(known_df['emotion'])\n",
    "train_texts = known_df['text'].tolist()\n",
    "predict_texts = predict_df['text'].tolist()\n",
    "\n",
    "print(f\"æ¨™ç±¤å°æ‡‰: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "\n",
    "print(\"\\nè¨“ç·´ RoBERTa-base(ä¿®æ­£ç‰ˆ:Seed+Weight Decay) ===\")\n",
    "\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            str(self.texts[idx]), \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            max_length=64, \n",
    "            return_attention_mask=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        label = self.labels[idx] if self.labels is not None else 0\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'][0], \n",
    "            'attention_mask': enc['attention_mask'][0], \n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = EmotionDataset(train_texts, y_full, tokenizer)\n",
    "predict_dataset = EmotionDataset(predict_texts, [0]*len(predict_texts), tokenizer)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(le.classes_))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./roberta_ensemble_final',\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.01,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    fp16=True,\n",
    "    save_strategy=\"no\", eval_strategy=\"no\", report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset)\n",
    "trainer.train()\n",
    "\n",
    "print(\"ç”ŸæˆRoBERTaé æ¸¬æ©Ÿç‡\")\n",
    "preds_output = trainer.predict(predict_dataset)\n",
    "probs_roberta = softmax(preds_output.predictions, axis=1)\n",
    "\n",
    "print(\"\\nè¨“ç·´SBERT(mpnet)+LR ===\")\n",
    "sbert = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "X_train_sbert = sbert.encode(train_texts, show_progress_bar=True)\n",
    "X_predict_sbert = sbert.encode(predict_texts, show_progress_bar=True)\n",
    "\n",
    "lr_model = LogisticRegression(class_weight='balanced', max_iter=1000, C=1.0, random_state=42)\n",
    "lr_model.fit(X_train_sbert, y_full)\n",
    "probs_sbert = lr_model.predict_proba(X_predict_sbert)\n",
    "\n",
    "print(\"\\nTF-IDF+XGBoost===\")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2), \n",
    "    min_df=2, \n",
    "    max_df=0.95\n",
    ")\n",
    "X_train_tfidf = tfidf.fit_transform(train_texts)\n",
    "X_predict_tfidf = tfidf.transform(predict_texts)\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=600, learning_rate=0.05, max_depth=4, \n",
    "    subsample=0.8, colsample_bytree=0.6,\n",
    "    objective=\"multi:softprob\", \n",
    "    num_class=len(le.classes_),\n",
    "    n_jobs=-1, random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train_tfidf, y_full)\n",
    "probs_xgboost = xgb_model.predict_proba(X_predict_tfidf)\n",
    "\n",
    "print(\"\\nåŸ·è¡ŒSoft Votingèåˆ\")\n",
    "\n",
    "#æ¬Šé‡åœ¨é€™è£¡èª¿æ•´\n",
    "w_roberta = 0.5\n",
    "w_sbert = 0.3\n",
    "w_xgboost = 0.2\n",
    "\n",
    "final_probs = (probs_roberta * w_roberta) + (probs_sbert * w_sbert) + (probs_xgboost * w_xgboost)\n",
    "final_pred_indices = np.argmax(final_probs, axis=1)\n",
    "final_labels = le.inverse_transform(final_pred_indices)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": predict_df[\"id\"],\n",
    "    \"emotion\": final_labels\n",
    "})\n",
    "\n",
    "filename = \"submission_soft_voting_optimized.csv\"\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"æœ€çµ‚ç‰ˆè¼¸å‡ºå®Œæˆï¼æª”æ¡ˆå·²å„²å­˜ç‚º: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a74f9a",
   "metadata": {},
   "source": [
    "### 5.14 DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46ae9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== å•Ÿå‹•æœ€çµ‚å…µå™¨: DeBERTa-v3-base ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é–‹å§‹è¨“ç·´ DeBERTa (é€™å¯èƒ½æœƒæ¯” RoBERTa æ…¢ä¸€é»)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4491' max='4491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4491/4491 2:22:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.265500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.961800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.903600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.792300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.784900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.703200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.684300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨é æ¸¬...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¼¸å‡ºå®Œæˆï¼è«‹ä¸Šå‚³: submission_deberta_v3_base.csv\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_full_encoded = le.fit_transform(known_df['emotion'])\n",
    "train_texts = known_df['text'].tolist()\n",
    "predict_texts = predict_df['text'].tolist()\n",
    "\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts; self.labels = labels; self.tokenizer = tokenizer\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(str(self.texts[idx]), truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
    "        label = self.labels[idx] if self.labels is not None else 0\n",
    "        return {'input_ids': enc['input_ids'][0], 'attention_mask': enc['attention_mask'][0], 'labels': torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "full_train_dataset = EmotionDataset(train_texts, y_full_encoded, tokenizer)\n",
    "predict_dataset = EmotionDataset(predict_texts, [0]*len(predict_texts), tokenizer)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=len(le.classes_),\n",
    "    use_safetensors=True, \n",
    "    weights_only=False, \n",
    "    trust_remote_code=True \n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./deberta_final',\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"no\", save_strategy=\"no\", report_to=\"none\", fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=full_train_dataset)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "preds = trainer.predict(predict_dataset)\n",
    "final_preds = le.inverse_transform(np.argmax(preds.predictions, axis=-1))\n",
    "\n",
    "submission = pd.DataFrame({\"id\": predict_df[\"id\"], \"emotion\": final_preds})\n",
    "filename = \"submission_deberta_v3_base.csv\"\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"è¼¸å‡ºå®Œæˆï¼è«‹ä¸Šå‚³: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce880bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ æ¨¡å‹å’Œ Tokenizer å·²æˆåŠŸå„²å­˜è‡³: ./deberta_best_score_model\n"
     ]
    }
   ],
   "source": [
    "#æŠŠè¨“ç·´å¥½çš„deBERTaæ¨¡å‹çµæœå­˜èµ·ä¾†ï¼Œé€™æ¨£ä¹‹å¾Œå°±ä¸ç”¨é‡è·‘å…©å°æ™‚\n",
    "MODEL_SAVE_PATH = \"./deberta_best_score_model\" \n",
    "\n",
    "trainer.save_model(MODEL_SAVE_PATH)\n",
    "tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"æ¨¡å‹å’ŒTokenizerå·²æˆåŠŸå„²å­˜è‡³: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f9f256",
   "metadata": {},
   "source": [
    "### 5.15.1 Soft Voting (85% DeBERTa + 15% LinearSVC (TF-IDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9237f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== å•Ÿå‹•çµ‚æ¥µèåˆ (åˆ©ç”¨è¨˜æ†¶é«”ä¸­çš„ DeBERTa) ===\n",
      "\n",
      "[1/3] æ­£åœ¨è¨“ç·´å‰¯æ‰‹æ¨¡å‹ï¼šCalibrated LinearSVC...\n",
      "   -> æå–æ‰‹å·¥ç‰¹å¾µ...\n",
      "   -> æå– TF-IDF ç‰¹å¾µ...\n",
      "   -> è¨“ç·´ SVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\omnl0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> SVM æº–å‚™å°±ç·’ã€‚\n",
      "\n",
      "[2/3] å¾è¨˜æ†¶é«”ä¸­æå– DeBERTa é æ¸¬æ©Ÿç‡...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> DeBERTa æ©Ÿç‡æå–å®Œæˆã€‚\n",
      "\n",
      "[3/3] åŸ·è¡ŒåŠ æ¬Šèåˆ (DeBERTa 0.85 + SVM 0.15)...\n",
      "ğŸ‰ æ¦œä¸€è¡åˆºæª”æ¡ˆå·²ç”Ÿæˆï¼šsubmission_deberta_svm.csv\n"
     ]
    }
   ],
   "source": [
    "train_texts_svc = known_df['text']\n",
    "predict_texts_svc = predict_df['text']\n",
    "y_full_svc = le.transform(known_df['emotion']) # ä½¿ç”¨åŸæœ¬çš„le\n",
    "\n",
    "train_feat = extract_features(train_texts_svc)\n",
    "pred_feat = extract_features(predict_texts_svc)\n",
    "\n",
    "scaler = RobustScaler() #æ¨™æº–åŒ–\n",
    "train_feat_sc = scaler.fit_transform(train_feat)\n",
    "pred_feat_sc = scaler.transform(pred_feat)\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), tokenizer=nltk.word_tokenize, token_pattern=None, min_df=2, max_df=0.95)\n",
    "train_tfidf = tfidf.fit_transform(train_texts_svc)\n",
    "pred_tfidf = tfidf.transform(predict_texts_svc)\n",
    "\n",
    "X_train_final = hstack([train_tfidf, train_feat_sc])\n",
    "X_predict_final = hstack([pred_tfidf, pred_feat_sc])\n",
    "\n",
    "svm = LinearSVC(C=0.15, class_weight='balanced', max_iter=3000, random_state=42)\n",
    "clf = CalibratedClassifierCV(svm, method='sigmoid', cv=5)\n",
    "clf.fit(X_train_final, y_full_svc)\n",
    "\n",
    "probs_svc = clf.predict_proba(X_predict_final)\n",
    "\n",
    "preds_output = trainer.predict(predict_dataset)\n",
    "\n",
    "probs_deberta = softmax(preds_output.predictions, axis=1)\n",
    "\n",
    "# è¨­å®šæ¬Šé‡\n",
    "w_deberta = 0.85\n",
    "w_svc = 0.15\n",
    "\n",
    "# è¨ˆç®—æœ€çµ‚æ©Ÿç‡\n",
    "final_probs = (probs_deberta*w_deberta) + (probs_svc*w_svc)\n",
    "final_pred_indices = np.argmax(final_probs, axis=1)\n",
    "final_labels = le.inverse_transform(final_pred_indices)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": predict_df[\"id\"],\n",
    "    \"emotion\": final_labels\n",
    "})\n",
    "\n",
    "filename = \"submission_deberta_svm.csv\"\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"æª”æ¡ˆå·²ç”Ÿæˆï¼š{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f3c25e",
   "metadata": {},
   "source": [
    "### 5.15.2 Soft Voting (80% DeBERTa + 20% LinearSVC (with TF-IDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3fb3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DeBERTa 0.80 + SVM 0.20 ===\n",
      "ğŸš€ æœ€çµ‚è³­æ³¨å·²ç”Ÿæˆï¼šsubmission_deberta_svm_adjust.csv\n",
      "æ¬Šé‡è¨­å®š -> DeBERTa: 0.8, SVM: 0.2\n"
     ]
    }
   ],
   "source": [
    "print(\"DeBERTa 0.80 + SVM 0.20\")\n",
    "\n",
    "w_deberta = 0.80\n",
    "w_svc = 0.20\n",
    "\n",
    "final_probs = (probs_deberta * w_deberta) + (probs_svc * w_svc)\n",
    "final_pred_indices = np.argmax(final_probs, axis=1)\n",
    "final_labels = le.inverse_transform(final_pred_indices)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": predict_df[\"id\"],\n",
    "    \"emotion\": final_labels\n",
    "})\n",
    "\n",
    "filename = \"submission_deberta_svm_adjust.csv\"\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"å·²ç”Ÿæˆï¼š{filename}\")\n",
    "print(f\"æ¬Šé‡è¨­å®š: DeBERTa: {w_deberta}, SVM: {w_svc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd447ea9",
   "metadata": {},
   "source": [
    "### 5.16 Psuedo Labeling (with DeBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06c4211",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output = trainer.predict(predict_dataset)\n",
    "probs = softmax(preds_output.predictions, axis=1)\n",
    "\n",
    "max_probs = np.max(probs, axis=1)\n",
    "pred_labels = np.argmax(probs, axis=1)\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.95 \n",
    "high_conf_indices = np.where(max_probs >= CONFIDENCE_THRESHOLD)[0]\n",
    "\n",
    "pseudo_texts = [predict_texts[i] for i in high_conf_indices]\n",
    "pseudo_labels = [pred_labels[i] for i in high_conf_indices]\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts; self.labels = labels; self.tokenizer = tokenizer\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(str(self.texts[idx]), truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
    "        return {'input_ids': enc['input_ids'][0], 'attention_mask': enc['attention_mask'][0], 'labels': torch.tensor(self.labels[idx], dtype=torch.long)}\n",
    "\n",
    "pseudo_dataset = EmotionDataset(pseudo_texts, pseudo_labels, tokenizer)\n",
    "original_train_dataset = EmotionDataset(train_texts, y_full_encoded, tokenizer)\n",
    "combined_train_dataset = ConcatDataset([original_train_dataset, pseudo_dataset])\n",
    "\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "model_pl = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=len(le.classes_),\n",
    "    use_safetensors=True, weights_only=False, trust_remote_code=True\n",
    ")\n",
    "\n",
    "training_args_pl = TrainingArguments(\n",
    "    output_dir='./deberta_pseudo_labeling',\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    save_strategy=\"no\", eval_strategy=\"no\", report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_pl = Trainer(\n",
    "    model=model_pl, \n",
    "    args=training_args_pl, \n",
    "    train_dataset=combined_train_dataset\n",
    ")\n",
    "\n",
    "trainer_pl.train()\n",
    "\n",
    "preds_output_pl = trainer_pl.predict(predict_dataset)\n",
    "final_preds_idx = np.argmax(preds_output_pl.predictions, axis=1)\n",
    "final_preds_label = le.inverse_transform(final_preds_idx)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": predict_df[\"id\"],\n",
    "    \"emotion\": final_preds_label\n",
    "})\n",
    "\n",
    "filename = \"submission_deberta_pseudo_labeling.csv\"\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"è¼¸å‡ºå®Œæˆï¼Œæª”æ¡ˆ: {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
